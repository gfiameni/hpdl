{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75280930",
   "metadata": {},
   "source": [
    "# Lab Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c77de",
   "metadata": {},
   "source": [
    "# Preparing and launching a DDP application\n",
    "Independent of how a DDP application is launched, each process needs a mechanism to know its global and local ranks. Once this is known, all\n",
    "processes create a `ProcessGroup` that enables them to participate in collective communication operations such as AllReduce.\n",
    "\n",
    "PyTorch has relatively simple interface for distributed training. To do distributed training, the model would just have to be wrapped using DistributedDataParallel and the training script would just have to be launched using torch.distributed.launch or torchrun, or leave the code spawn multiple processes. \n",
    "\n",
    "This set of examples presents simple implementations of distributed training or message passings: \n",
    "1. CIFAR-10 classification using DistributedDataParallel wrapped ResNet models\n",
    "2. ToyModel using multiprocessing ingerface\n",
    "3. Message passing topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fe20",
   "metadata": {},
   "source": [
    "## Example 1 (CIFAR-10 Classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855a4be",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"resnet_ddp.py\"\"\"\n",
    "\n",
    "def main():\n",
    "\n",
    "    num_epochs_default = 5\n",
    "    batch_size_default = 256 # 1024\n",
    "    learning_rate_default = 0.1\n",
    "    random_seed_default = 0\n",
    "    model_dir_default = \"saved_models\"\n",
    "    model_filename_default = \"resnet_distributed.pth\"\n",
    "\n",
    "    # Each process runs on 1 GPU device specified by the local_rank argument.\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--local_rank\", type=int, help=\"Local rank. Necessary for using the torch.distributed.launch utility.\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, help=\"Number of training epochs.\", default=num_epochs_default)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Training batch size for one process.\", default=batch_size_default)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate.\", default=learning_rate_default)\n",
    "    parser.add_argument(\"--random_seed\", type=int, help=\"Random seed.\", default=random_seed_default)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=model_dir_default)\n",
    "    parser.add_argument(\"--model_filename\", type=str, help=\"Model filename.\", default=model_filename_default)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from saved checkpoint.\")\n",
    "    argv = parser.parse_args()\n",
    "\n",
    "    local_rank = argv.local_rank\n",
    "    num_epochs = argv.num_epochs\n",
    "    batch_size = argv.batch_size\n",
    "    learning_rate = argv.learning_rate\n",
    "    random_seed = argv.random_seed\n",
    "    model_dir = argv.model_dir\n",
    "    model_filename = argv.model_filename\n",
    "    resume = argv.resume\n",
    "\n",
    "    if local_rank is None:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # Create directories outside the PyTorch program\n",
    "    # Do not create directory here because it is not multiprocess safe\n",
    "    '''\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    '''\n",
    "\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "    # We need to use seeds to make sure that the models initialized in different processes are the same\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "    \n",
    "    # setup(local_rank, world_size)\n",
    "\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    # torch.distributed.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    # Encapsulate the model on the GPU assigned to the current process\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "    model = model.to(device)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    # We only save the model who uses device \"cuda:0\"\n",
    "    # To resume, the device for the saved model would also be \"cuda:0\"\n",
    "    if resume == True:\n",
    "        map_location = {\"cuda:0\": \"cuda:{}\".format(local_rank)}\n",
    "        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Data should be prefetched\n",
    "    # Download should be set to be False, because it is not multiprocess safe\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=True, download=True, transform=transform) \n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Restricts data loading to a subset of the dataset exclusive to the current process\n",
    "    train_sampler = DistributedSampler(dataset=train_set)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
    "    # Test loader does not have to follow distributed sampling strategy\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    # Loop over the dataset multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t0 = time.time()\n",
    "        # Save and evaluate model routinely\n",
    "        if epoch % 1 == 0:\n",
    "            if local_rank == 0:\n",
    "                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)\n",
    "                torch.save(ddp_model.state_dict(), model_filepath)\n",
    "                print(\"-\" * 75)\n",
    "                print(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "                print(\"-\" * 75)\n",
    "\n",
    "        ddp_model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Local Rank: {}, Epoch: {}, Training ...\".format(local_rank, epoch))\n",
    "        print(\"Time {} seconds\".format(time.time() - t0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e25f4",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "- Use --local_rank for argparse if we are going to use torch.distributed.launch to launch distributed training.\n",
    "- Set random seed to make sure that the models initialized in different processes are the same. \n",
    "- Use DistributedDataParallel to wrap the model for distributed training.\n",
    "- Use DistributedSampler to training data loader.\n",
    "- To save models, each node would save a copy of the checkpoint file in the local hard drive.\n",
    "- Downloading dataset and making directories should be avoided in the distributed training program as they are not multi-process safe, unless we use some sort of barriers, such as torch.distributed.barrier.\n",
    "- The node communication bandwidth are extremely important for multi-node distributed training. Instead of randomly finding two computers in the network, try to use the nodes from the specialized computing clusters, since the communications between the nodes are highly optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f20e5",
   "metadata": {},
   "source": [
    "### Get familiar with the system architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88485e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e366cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628f2a2",
   "metadata": {},
   "source": [
    "### Distributed launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0380c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m torch.distributed.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feede48c",
   "metadata": {},
   "source": [
    "#### Launch command 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=1234 resnet_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca885a7",
   "metadata": {},
   "source": [
    "### Torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98cdc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torchrun [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                [--max_restarts MAX_RESTARTS]\n",
      "                [--monitor_interval MONITOR_INTERVAL]\n",
      "                [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                [--master_port MASTER_PORT]\n",
      "                training_script ...\n",
      "torchrun: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "! torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d14cb",
   "metadata": {},
   "source": [
    "#### Launch command 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35330a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 torchrun --standalone --nnodes=1 --nproc_per_node=2 resnet_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27461832",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c924f6",
   "metadata": {},
   "source": [
    "## Example 2 (ToyModel using multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ed634",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"mp.py\"\"\"\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d01f6b",
   "metadata": {},
   "source": [
    "#### Launch command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ac5ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic DDP example on rank 0.\n",
      "Running basic DDP example on rank 1.\n",
      "Traceback (most recent call last):\n",
      "  File \"mp.py\", line 153, in <module>\n",
      "    run_demo(demo_basic, 2)\n",
      "  File \"mp.py\", line 57, in run_demo\n",
      "    mp.spawn(demo_fn,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/workspace/ai-school/mp.py\", line 38, in demo_basic\n",
      "    setup(rank, world_size)\n",
      "  File \"/workspace/ai-school/mp.py\", line 18, in setup\n",
      "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 577, in init_process_group\n",
      "    store, rank, world_size = next(rendezvous_iterator)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n",
      "    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n",
      "    return TCPStore(\n",
      "RuntimeError: Address already in use\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 python mp.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10749c9e",
   "metadata": {},
   "source": [
    "**What is wrong with the following command?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646c8ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Running basic DDP example on rank 0.\n",
      "Running basic DDP example on rank 0.\n",
      "Running basic DDP example on rank 1.\n",
      "Running basic DDP example on rank 1.\n",
      "Traceback (most recent call last):\n",
      "  File \"mp.py\", line 153, in <module>\n",
      "    run_demo(demo_basic, 2)\n",
      "  File \"mp.py\", line 57, in run_demo\n",
      "    mp.spawn(demo_fn,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/workspace/ai-school/mp.py\", line 38, in demo_basic\n",
      "    setup(rank, world_size)\n",
      "  File \"/workspace/ai-school/mp.py\", line 18, in setup\n",
      "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 577, in init_process_group\n",
      "    store, rank, world_size = next(rendezvous_iterator)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n",
      "    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n",
      "    return TCPStore(\n",
      "RuntimeError: Address already in use\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"mp.py\", line 153, in <module>\n",
      "    run_demo(demo_basic, 2)\n",
      "  File \"mp.py\", line 57, in run_demo\n",
      "    mp.spawn(demo_fn,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/workspace/ai-school/mp.py\", line 38, in demo_basic\n",
      "    setup(rank, world_size)\n",
      "  File \"/workspace/ai-school/mp.py\", line 18, in setup\n",
      "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 577, in init_process_group\n",
      "    store, rank, world_size = next(rendezvous_iterator)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n",
      "    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n",
      "    return TCPStore(\n",
      "RuntimeError: Address already in use\n",
      "\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11351) of binary: /opt/conda/bin/python\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:370: UserWarning: \n",
      "\n",
      "**********************************************************************\n",
      "               CHILD PROCESS FAILED WITH NO ERROR_FILE                \n",
      "**********************************************************************\n",
      "CHILD PROCESS FAILED WITH NO ERROR_FILE\n",
      "Child process 11351 (local_rank 0) FAILED (exitcode 1)\n",
      "Error msg: Process failed with exitcode 1\n",
      "Without writing an error file to <N/A>.\n",
      "While this DOES NOT affect the correctness of your application,\n",
      "no trace information about the error will be available for inspection.\n",
      "Consider decorating your top level entrypoint function with\n",
      "torch.distributed.elastic.multiprocessing.errors.record. Example:\n",
      "\n",
      "  from torch.distributed.elastic.multiprocessing.errors import record\n",
      "\n",
      "  @record\n",
      "  def trainer_main(args):\n",
      "     # do train\n",
      "**********************************************************************\n",
      "  warnings.warn(_no_error_file_warning_msg(rank, failure))\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==1.10.0a0+0aef44c', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 364, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 698, in main\n",
      "    run(args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 689, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 259, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "***************************************\n",
      "              mp.py FAILED             \n",
      "=======================================\n",
      "Root Cause:\n",
      "[0]:\n",
      "  time: 2022-02-22_10:54:55\n",
      "  rank: 0 (local_rank: 0)\n",
      "  exitcode: 1 (pid: 11351)\n",
      "  error_file: <N/A>\n",
      "  msg: \"Process failed with exitcode 1\"\n",
      "=======================================\n",
      "Other Failures:\n",
      "[1]:\n",
      "  time: 2022-02-22_10:54:55\n",
      "  rank: 1 (local_rank: 1)\n",
      "  exitcode: 1 (pid: 11352)\n",
      "  error_file: <N/A>\n",
      "  msg: \"Process failed with exitcode 1\"\n",
      "***************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 torchrun --standalone --nnodes=1 --nproc_per_node=2 mp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d8df2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2846ca",
   "metadata": {},
   "source": [
    "## Example 3 (Message passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444257b7",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"send_receive.py\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "\n",
    "def init_processes(rank, size, fn, backend='tcp'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run_non_blocking)) # Options: run_non_blocking, run_blocking, run_allreduce\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "```\n",
    "\n",
    "The above script spawns two processes who will each setup the distributed environment, initialize the process group (`dist.init_process_group`), and finally execute the given `run` function. \n",
    "\n",
    "The `init_processes` ensures that every process will be able to coordinate through a master, using the same ip address and port. Note that we used the TCP backend, but we could have used [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) or [Gloo](http://github.com/facebookincubator/gloo) instead. \n",
    "\n",
    "### Point-to-Point Communication\n",
    "<!--\n",
    "* send/recv\n",
    "* isend/irecv\n",
    "-->\n",
    "\n",
    "![alt text](./figs/send_recv.png)\n",
    "\n",
    "\n",
    "A transfer of data from one process to another is called a point-to-point communication. These are achieved through the `send` and `recv` functions or their *immediate* counter-parts, `isend` and `irecv`.\n",
    "\n",
    "```python\n",
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "\n",
    "In the above example, both processes start with a zero tensor, then process 0 increments the tensor and sends it to process 1 so that they both end up with 1.0. Notice that process 1 needs to allocate memory in order to store the data it will receive.\n",
    "\n",
    "Also notice that `send`/`recv` are **blocking**: both processes stop until the communication is completed. On the other hand immediates are **non-blocking**; the script continues its execution and the methods return a `DistributedRequest` object upon which we can choose to `wait()`.\n",
    "\n",
    "```python\n",
    "\"\"\"Non-blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_non_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        req = dist.isend(tensor=tensor, dst=1)\n",
    "        print('Rank 0 started sending')\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print('Rank 1 started receiving')\n",
    "        print('Rank 1 has data ', tensor[0])\n",
    "    req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "\n",
    "Running the above function might result in process 1 still having 0.0 while having already started receiving. However, after `req.wait()` has been executed we are guaranteed that the communication took place, and that the value stored in `tensor[0]` is 1.0.\n",
    "\n",
    "Point-to-point communication is useful when we want a fine-grained control over the communication of our processes. They can be used to implement fancy algorithms, such as the one used in [Baidu's DeepSpeech](https://github.com/baidu-research/baidu-allreduce) or [Facebook's large-scale experiments](https://research.fb.com/publications/imagenet1kin1h/).)\n",
    "\n",
    "### Collective Communication\n",
    "<!--\n",
    "* gather\n",
    "* reduce\n",
    "* broadcast\n",
    "* scatter\n",
    "* all_reduce\n",
    "-->\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/scatter.png' width=100% /><br/>\n",
    "<b>Broadcast</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/all_gather.png' width=100% /><br/>\n",
    "<b>AllGather</b>\n",
    "</td>\n",
    "\n",
    "</tr><tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/reduce.png' width=100% /><br/>\n",
    "<b>Reduce</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/all_reduce.png' width=100% /><br/>\n",
    "<b>AllReduce</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/scatter.png' width=100% /><br/>\n",
    "<b>Scatter</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/gather.png' width=100% /><br/>\n",
    "<b>Gather</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a **group**. A group is a subset of all our processes. To create a group, we can pass a list of ranks to `dist.new_group(group)`. By default, collectives are executed on the all processes, also known as the **world**. For example, in order to obtain the sum of all tensors at all processes, we can use the `dist.all_reduce(tensor, op, group)` collective.\n",
    "\n",
    "```python\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "def run_all_reduce(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    group = dist.new_group([0, 1]) \n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "```\n",
    "\n",
    "Since we want the sum of all tensors in the group, we use `dist.reduce_op.SUM` as the reduce operator. Generally speaking, any commutative mathematical operation can be used as an operator. Out-of-the-box, PyTorch comes with 4 such operators, all working at the element-wise level:\n",
    "\n",
    "* `dist.reduce_op.SUM`,\n",
    "* `dist.reduce_op.PRODUCT`,\n",
    "* `dist.reduce_op.MAX`,\n",
    "* `dist.reduce_op.MIN`.\n",
    "\n",
    "In addition to `dist.all_reduce(tensor, op, group)`, there are a total of 6 collectives currently implemented in PyTorch.\n",
    "\n",
    "* `dist.broadcast(tensor, src, group)`: Copies `tensor` from `src` to all other processes.\n",
    "* `dist.reduce(tensor, dst, op, group)`: Applies `op` to all `tensor` and stores the result in `dst`.\n",
    "* `dist.all_reduce(tensor, op, group)`: Same as reduce, but the result is stored in all processes.\n",
    "* `dist.scatter(tensor, src, scatter_list, group)`: Copies the $i^{\\text{th}}$ tensor `scatter_list[i]` to the $i^{\\text{th}}$ process.\n",
    "* `dist.gather(tensor, dst, gather_list, group)`: Copies `tensor` from all processes in `dst`.\n",
    "* `dist.all_gather(tensor_list, tensor, group)`: Copies `tensor` from all processes to `tensor_list`, on all processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05008535",
   "metadata": {},
   "source": [
    "#### Launch command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6620952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python send_receive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a6314",
   "metadata": {},
   "source": [
    "## Useful links\n",
    "- [https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)\n",
    "- [https://leimao.github.io/blog/PyTorch-Distributed-Training/](https://leimao.github.io/blog/PyTorch-Distributed-Training/)\n",
    "- [https://pytorch.org/tutorials/intermediate/ddp_tutorial.html](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
    "- [https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "- [https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe6bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
