{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26177522",
   "metadata": {},
   "source": [
    "# Lab Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28b786",
   "metadata": {},
   "source": [
    "# Preparing and launching a DDP application\n",
    "Independent of how a DDP application is launched, each process needs a mechanism to know its global and local ranks. Once this is known, all\n",
    "processes create a `ProcessGroup` that enables them to participate in collective communication operations such as AllReduce.\n",
    "\n",
    "PyTorch has relatively simple interface for distributed training. To do distributed training, the model would just have to be wrapped using DistributedDataParallel and the training script would just have to be launched using torch.distributed.launch or torchrun, or leave the code spawn multiple processes. \n",
    "\n",
    "This set of examples presents simple implementations of distributed training or message passings: \n",
    "1. CIFAR-10 classification using DistributedDataParallel wrapped ResNet models\n",
    "2. ToyModel using multiprocessing ingerface\n",
    "3. Message passing topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38de42d",
   "metadata": {},
   "source": [
    "## Example 1 (CIFAR-10 Classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8229df0",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"resnet_ddp.py\"\"\"\n",
    "\n",
    "def main():\n",
    "\n",
    "    num_epochs_default = 5\n",
    "    batch_size_default = 256 # 1024\n",
    "    learning_rate_default = 0.1\n",
    "    random_seed_default = 0\n",
    "    model_dir_default = \"saved_models\"\n",
    "    model_filename_default = \"resnet_distributed.pth\"\n",
    "\n",
    "    # Each process runs on 1 GPU device specified by the local_rank argument.\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--local_rank\", type=int, help=\"Local rank. Necessary for using the torch.distributed.launch utility.\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, help=\"Number of training epochs.\", default=num_epochs_default)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Training batch size for one process.\", default=batch_size_default)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate.\", default=learning_rate_default)\n",
    "    parser.add_argument(\"--random_seed\", type=int, help=\"Random seed.\", default=random_seed_default)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=model_dir_default)\n",
    "    parser.add_argument(\"--model_filename\", type=str, help=\"Model filename.\", default=model_filename_default)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from saved checkpoint.\")\n",
    "    argv = parser.parse_args()\n",
    "\n",
    "    local_rank = argv.local_rank\n",
    "    num_epochs = argv.num_epochs\n",
    "    batch_size = argv.batch_size\n",
    "    learning_rate = argv.learning_rate\n",
    "    random_seed = argv.random_seed\n",
    "    model_dir = argv.model_dir\n",
    "    model_filename = argv.model_filename\n",
    "    resume = argv.resume\n",
    "\n",
    "    if local_rank is None:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # Create directories outside the PyTorch program\n",
    "    # Do not create directory here because it is not multiprocess safe\n",
    "    '''\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    '''\n",
    "\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "    # We need to use seeds to make sure that the models initialized in different processes are the same\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "    \n",
    "    # setup(local_rank, world_size)\n",
    "\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    # torch.distributed.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    # Encapsulate the model on the GPU assigned to the current process\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "    model = model.to(device)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    # We only save the model who uses device \"cuda:0\"\n",
    "    # To resume, the device for the saved model would also be \"cuda:0\"\n",
    "    if resume == True:\n",
    "        map_location = {\"cuda:0\": \"cuda:{}\".format(local_rank)}\n",
    "        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Data should be prefetched\n",
    "    # Download should be set to be False, because it is not multiprocess safe\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=True, download=True, transform=transform) \n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Restricts data loading to a subset of the dataset exclusive to the current process\n",
    "    train_sampler = DistributedSampler(dataset=train_set)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
    "    # Test loader does not have to follow distributed sampling strategy\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    # Loop over the dataset multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t0 = time.time()\n",
    "        # Save and evaluate model routinely\n",
    "        if epoch % 1 == 0:\n",
    "            if local_rank == 0:\n",
    "                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)\n",
    "                torch.save(ddp_model.state_dict(), model_filepath)\n",
    "                print(\"-\" * 75)\n",
    "                print(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "                print(\"-\" * 75)\n",
    "\n",
    "        ddp_model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Local Rank: {}, Epoch: {}, Training ...\".format(local_rank, epoch))\n",
    "        print(\"Time {} seconds\".format(time.time() - t0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d4a01",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "- Use --local_rank for argparse if we are going to use torch.distributed.launch to launch distributed training.\n",
    "- Set random seed to make sure that the models initialized in different processes are the same. \n",
    "- Use DistributedDataParallel to wrap the model for distributed training.\n",
    "- Use DistributedSampler to training data loader.\n",
    "- To save models, each node would save a copy of the checkpoint file in the local hard drive.\n",
    "- Downloading dataset and making directories should be avoided in the distributed training program as they are not multi-process safe, unless we use some sort of barriers, such as torch.distributed.barrier.\n",
    "- The node communication bandwidth are extremely important for multi-node distributed training. Instead of randomly finding two computers in the network, try to use the nodes from the specialized computing clusters, since the communications between the nodes are highly optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e40ace",
   "metadata": {},
   "source": [
    "### Get familiar with the system architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c34fd1",
   "metadata": {},
   "source": [
    "### Distributed launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m torch.distributed.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72611c04",
   "metadata": {},
   "source": [
    "#### Launch command 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=1234 resnet_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e9744",
   "metadata": {},
   "source": [
    "### Torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e49d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23b8a0",
   "metadata": {},
   "source": [
    "#### Launch command 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cae3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 torchrun --standalone --nnodes=1 --nproc_per_node=2 resnet_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8967cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f97e55",
   "metadata": {},
   "source": [
    "## Example 2 (ToyModel using multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304fa484",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"mp.py\"\"\"\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d5aa4",
   "metadata": {},
   "source": [
    "#### Launch command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 python mp.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9242d",
   "metadata": {},
   "source": [
    "**What is wrong with the following command?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0cfa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 torchrun --standalone --nnodes=1 --nproc_per_node=2 mp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed9531",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f21ca",
   "metadata": {},
   "source": [
    "## Example 3 (Message passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7565e1a",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"send_receive.py\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "\n",
    "def init_processes(rank, size, fn, backend='tcp'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run_non_blocking)) # Options: run_non_blocking, run_blocking, run_allreduce\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "```\n",
    "\n",
    "The above script spawns two processes who will each setup the distributed environment, initialize the process group (`dist.init_process_group`), and finally execute the given `run` function. \n",
    "\n",
    "The `init_processes` ensures that every process will be able to coordinate through a master, using the same ip address and port. Note that we used the TCP backend, but we could have used [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) or [Gloo](http://github.com/facebookincubator/gloo) instead. \n",
    "\n",
    "### Point-to-Point Communication\n",
    "<!--\n",
    "* send/recv\n",
    "* isend/irecv\n",
    "-->\n",
    "\n",
    "![alt text](./figs/send_recv.png)\n",
    "\n",
    "\n",
    "A transfer of data from one process to another is called a point-to-point communication. These are achieved through the `send` and `recv` functions or their *immediate* counter-parts, `isend` and `irecv`.\n",
    "\n",
    "```python\n",
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "\n",
    "In the above example, both processes start with a zero tensor, then process 0 increments the tensor and sends it to process 1 so that they both end up with 1.0. Notice that process 1 needs to allocate memory in order to store the data it will receive.\n",
    "\n",
    "Also notice that `send`/`recv` are **blocking**: both processes stop until the communication is completed. On the other hand immediates are **non-blocking**; the script continues its execution and the methods return a `DistributedRequest` object upon which we can choose to `wait()`.\n",
    "\n",
    "```python\n",
    "\"\"\"Non-blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_non_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        req = dist.isend(tensor=tensor, dst=1)\n",
    "        print('Rank 0 started sending')\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print('Rank 1 started receiving')\n",
    "        print('Rank 1 has data ', tensor[0])\n",
    "    req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "\n",
    "Running the above function might result in process 1 still having 0.0 while having already started receiving. However, after `req.wait()` has been executed we are guaranteed that the communication took place, and that the value stored in `tensor[0]` is 1.0.\n",
    "\n",
    "Point-to-point communication is useful when we want a fine-grained control over the communication of our processes. They can be used to implement fancy algorithms, such as the one used in [Baidu's DeepSpeech](https://github.com/baidu-research/baidu-allreduce) or [Facebook's large-scale experiments](https://research.fb.com/publications/imagenet1kin1h/).)\n",
    "\n",
    "### Collective Communication\n",
    "<!--\n",
    "* gather\n",
    "* reduce\n",
    "* broadcast\n",
    "* scatter\n",
    "* all_reduce\n",
    "-->\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/scatter.png' width=100% /><br/>\n",
    "<b>Broadcast</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/all_gather.png' width=100% /><br/>\n",
    "<b>AllGather</b>\n",
    "</td>\n",
    "\n",
    "</tr><tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/reduce.png' width=100% /><br/>\n",
    "<b>Reduce</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/all_reduce.png' width=100% /><br/>\n",
    "<b>AllReduce</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/scatter.png' width=100% /><br/>\n",
    "<b>Scatter</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/gather.png' width=100% /><br/>\n",
    "<b>Gather</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a **group**. A group is a subset of all our processes. To create a group, we can pass a list of ranks to `dist.new_group(group)`. By default, collectives are executed on the all processes, also known as the **world**. For example, in order to obtain the sum of all tensors at all processes, we can use the `dist.all_reduce(tensor, op, group)` collective.\n",
    "\n",
    "```python\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "def run_all_reduce(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    group = dist.new_group([0, 1]) \n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "```\n",
    "\n",
    "Since we want the sum of all tensors in the group, we use `dist.reduce_op.SUM` as the reduce operator. Generally speaking, any commutative mathematical operation can be used as an operator. Out-of-the-box, PyTorch comes with 4 such operators, all working at the element-wise level:\n",
    "\n",
    "* `dist.reduce_op.SUM`,\n",
    "* `dist.reduce_op.PRODUCT`,\n",
    "* `dist.reduce_op.MAX`,\n",
    "* `dist.reduce_op.MIN`.\n",
    "\n",
    "In addition to `dist.all_reduce(tensor, op, group)`, there are a total of 6 collectives currently implemented in PyTorch.\n",
    "\n",
    "* `dist.broadcast(tensor, src, group)`: Copies `tensor` from `src` to all other processes.\n",
    "* `dist.reduce(tensor, dst, op, group)`: Applies `op` to all `tensor` and stores the result in `dst`.\n",
    "* `dist.all_reduce(tensor, op, group)`: Same as reduce, but the result is stored in all processes.\n",
    "* `dist.scatter(tensor, src, scatter_list, group)`: Copies the $i^{\\text{th}}$ tensor `scatter_list[i]` to the $i^{\\text{th}}$ process.\n",
    "* `dist.gather(tensor, dst, gather_list, group)`: Copies `tensor` from all processes in `dst`.\n",
    "* `dist.all_gather(tensor_list, tensor, group)`: Copies `tensor` from all processes to `tensor_list`, on all processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef16286",
   "metadata": {},
   "source": [
    "#### Launch command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python send_receive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38ab8d",
   "metadata": {},
   "source": [
    "## Useful links\n",
    "- [https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)\n",
    "- [https://leimao.github.io/blog/PyTorch-Distributed-Training/](https://leimao.github.io/blog/PyTorch-Distributed-Training/)\n",
    "- [https://pytorch.org/tutorials/intermediate/ddp_tutorial.html](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
    "- [https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "- [https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)\n",
    "- [https://huggingface.co/docs/transformers/performance](https://huggingface.co/docs/transformers/performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3e996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
