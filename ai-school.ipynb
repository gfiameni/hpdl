{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47367a6f",
   "metadata": {},
   "source": [
    "# PyTorch Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d7c70",
   "metadata": {},
   "source": [
    "# Preparing and launching a DDP application\n",
    "Independent of how a DDP application is launched, each process needs a mechanism to know its global and local ranks. Once this is known, all\n",
    "processes create a `ProcessGroup` that enables them to participate in collective communication operations such as AllReduce.\n",
    "\n",
    "PyTorch has relatively simple interface for distributed training. To do distributed training, the model would just have to be wrapped using DistributedDataParallel and the training script would just have to be launched using torch.distributed.launch or torchrun, or leave the code spawn multiple processes. \n",
    "\n",
    "This examples presents a simple implementation of PyTorch distributed training on CIFAR-10 classification using DistributedDataParallel wrapped ResNet models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4aea3e",
   "metadata": {},
   "source": [
    "## Examples (Distributed Data Parallel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738561da",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def main():\n",
    "\n",
    "    num_epochs_default = 5\n",
    "    batch_size_default = 256 # 1024\n",
    "    learning_rate_default = 0.1\n",
    "    random_seed_default = 0\n",
    "    model_dir_default = \"saved_models\"\n",
    "    model_filename_default = \"resnet_distributed.pth\"\n",
    "\n",
    "    # Each process runs on 1 GPU device specified by the local_rank argument.\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--local_rank\", type=int, help=\"Local rank. Necessary for using the torch.distributed.launch utility.\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, help=\"Number of training epochs.\", default=num_epochs_default)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Training batch size for one process.\", default=batch_size_default)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate.\", default=learning_rate_default)\n",
    "    parser.add_argument(\"--random_seed\", type=int, help=\"Random seed.\", default=random_seed_default)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=model_dir_default)\n",
    "    parser.add_argument(\"--model_filename\", type=str, help=\"Model filename.\", default=model_filename_default)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from saved checkpoint.\")\n",
    "    argv = parser.parse_args()\n",
    "\n",
    "    local_rank = argv.local_rank\n",
    "    num_epochs = argv.num_epochs\n",
    "    batch_size = argv.batch_size\n",
    "    learning_rate = argv.learning_rate\n",
    "    random_seed = argv.random_seed\n",
    "    model_dir = argv.model_dir\n",
    "    model_filename = argv.model_filename\n",
    "    resume = argv.resume\n",
    "\n",
    "    if local_rank is None:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # Create directories outside the PyTorch program\n",
    "    # Do not create directory here because it is not multiprocess safe\n",
    "    '''\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    '''\n",
    "\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "    # We need to use seeds to make sure that the models initialized in different processes are the same\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "    \n",
    "    # setup(local_rank, world_size)\n",
    "\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    # torch.distributed.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    # Encapsulate the model on the GPU assigned to the current process\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "    model = model.to(device)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    # We only save the model who uses device \"cuda:0\"\n",
    "    # To resume, the device for the saved model would also be \"cuda:0\"\n",
    "    if resume == True:\n",
    "        map_location = {\"cuda:0\": \"cuda:{}\".format(local_rank)}\n",
    "        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Data should be prefetched\n",
    "    # Download should be set to be False, because it is not multiprocess safe\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=True, download=True, transform=transform) \n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Restricts data loading to a subset of the dataset exclusive to the current process\n",
    "    train_sampler = DistributedSampler(dataset=train_set)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
    "    # Test loader does not have to follow distributed sampling strategy\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    # Loop over the dataset multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t0 = time.time()\n",
    "        # Save and evaluate model routinely\n",
    "        if epoch % 1 == 0:\n",
    "            if local_rank == 0:\n",
    "                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)\n",
    "                torch.save(ddp_model.state_dict(), model_filepath)\n",
    "                print(\"-\" * 75)\n",
    "                print(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "                print(\"-\" * 75)\n",
    "\n",
    "        ddp_model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Local Rank: {}, Epoch: {}, Training ...\".format(local_rank, epoch))\n",
    "        print(\"Time {} seconds\".format(time.time() - t0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f650b",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "The caveats are as the follows:\n",
    "- Use --local_rank for argparse if we are going to use torch.distributed.launch to launch distributed training.\n",
    "- Set random seed to make sure that the models initialized in different processes are the same. \n",
    "- Use DistributedDataParallel to wrap the model for distributed training.\n",
    "- Use DistributedSampler to training data loader.\n",
    "- To save models, each node would save a copy of the checkpoint file in the local hard drive.\n",
    "- Downloading dataset and making directories should be avoided in the distributed training program as they are not multi-process safe, unless we use some sort of barriers, such as torch.distributed.barrier.\n",
    "- The node communication bandwidth are extremely important for multi-node distributed training. Instead of randomly finding two computers in the network, try to use the nodes from the specialized computing clusters, since the communications between the nodes are highly optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3beda8",
   "metadata": {},
   "source": [
    "### Launching Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930d4e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\u001b[0m\n",
      "GPU0\t X \tNV1\tNV1\tNV2\t0-39\n",
      "GPU1\tNV1\t X \tNV2\tNV1\t0-39\n",
      "GPU2\tNV1\tNV2\t X \tNV1\t0-39\n",
      "GPU3\tNV2\tNV1\tNV1\t X \t0-39\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing a single PCIe switch\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f709e618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 18 16:30:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   58C    P0   248W / 300W |  31196MiB / 32478MiB |     99%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   57C    P0   277W / 300W |   6882MiB / 32478MiB |     98%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   51C    P0    63W / 300W |   4334MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   51C    P0    59W / 300W |  31613MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1fab31",
   "metadata": {},
   "source": [
    "### Distributed launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df5663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "usage: launch.py [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                 [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                 [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                 [--max_restarts MAX_RESTARTS]\n",
      "                 [--monitor_interval MONITOR_INTERVAL]\n",
      "                 [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                 [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                 [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                 [--master_port MASTER_PORT] [--use_env]\n",
      "                 training_script ...\n",
      "launch.py: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "! python -m torch.distributed.launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45aff825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 0, Accuracy: 0.0\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 1, Epoch: 0, Training ...\n",
      "Time 9.920054197311401 seconds\n",
      "Local Rank: 0, Epoch: 0, Training ...\n",
      "Time 9.921112060546875 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 1, Accuracy: 0.3024\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 1, Training ...\n",
      "Time 7.2499260902404785 seconds\n",
      "Local Rank: 1, Epoch: 1, Training ...\n",
      "Time 7.2615966796875 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 2, Accuracy: 0.4029\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 2, Training ...\n",
      "Time 7.2188451290130615 seconds\n",
      "Local Rank: 1, Epoch: 2, Training ...\n",
      "Time 7.211968898773193 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 3, Accuracy: 0.4441\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 3, Training ...\n",
      "Time 7.235145330429077 seconds\n",
      "Local Rank: 1, Epoch: 3, Training ...\n",
      "Time 7.23486590385437 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 4, Accuracy: 0.5027\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 4, Training ...\n",
      "Time 7.096554756164551 seconds\n",
      "Local Rank: 1, Epoch: 4, Training ...\n",
      "Time 7.101935148239136 seconds\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=1234 resnet_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e7829",
   "metadata": {},
   "source": [
    "### Torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3266117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torchrun [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                [--max_restarts MAX_RESTARTS]\n",
      "                [--monitor_interval MONITOR_INTERVAL]\n",
      "                [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                [--master_port MASTER_PORT]\n",
      "                training_script ...\n",
      "torchrun: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "! torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cefc807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 0, Accuracy: 0.0\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 1, Epoch: 0, Training ...\n",
      "Time 10.598102807998657 seconds\n",
      "Local Rank: 0, Epoch: 0, Training ...\n",
      "Time 10.581548929214478 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 1, Accuracy: 0.3024\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 1, Epoch: 1, Training ...\n",
      "Time 7.2175726890563965 seconds\n",
      "Local Rank: 0, Epoch: 1, Training ...\n",
      "Time 7.211953401565552 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 2, Accuracy: 0.4029\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 2, Training ...\n",
      "Time 7.1010167598724365 seconds\n",
      "Local Rank: 1, Epoch: 2, Training ...\n",
      "Time 7.1082923412323 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 3, Accuracy: 0.4441\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 1, Epoch: 3, Training ...\n",
      "Time 7.007239818572998 seconds\n",
      "Local Rank: 0, Epoch: 3, Training ...\n",
      "Time 7.017667531967163 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 4, Accuracy: 0.5027\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 4, Training ...\n",
      "Time 6.973626613616943 seconds\n",
      "Local Rank: 1, Epoch: 4, Training ...\n",
      "Time 6.98218297958374 seconds\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 torchrun --standalone --nnodes=1 --nproc_per_node=2 resnet_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581a116",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cfafb",
   "metadata": {},
   "source": [
    "## Example (multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d4fae",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825b9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic DDP example on rank 1.\n",
      "Running basic DDP example on rank 0.\n",
      "Running DDP checkpoint example on rank 0.\n",
      "Running DDP checkpoint example on rank 1.\n",
      "Running DDP with model parallel example on rank 0.\n",
      "Running DDP with model parallel example on rank 1.\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 python mp.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c942e4e",
   "metadata": {},
   "source": [
    "**What is wrong with the following command?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c071a644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Running basic DDP example on rank 0.\n",
      "Running basic DDP example on rank 1.\n",
      "Running basic DDP example on rank 0.\n",
      "Traceback (most recent call last):\n",
      "  File \"mp.py\", line 153, in <module>\n",
      "    run_demo(demo_basic, 2)\n",
      "  File \"mp.py\", line 57, in run_demo\n",
      "    mp.spawn(demo_fn,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/workspace/ai-school/mp.py\", line 38, in demo_basic\n",
      "    setup(rank, world_size)\n",
      "  File \"/workspace/ai-school/mp.py\", line 18, in setup\n",
      "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 577, in init_process_group\n",
      "    store, rank, world_size = next(rendezvous_iterator)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n",
      "    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n",
      "    return TCPStore(\n",
      "RuntimeError: Address already in use\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"mp.py\", line 153, in <module>\n",
      "    run_demo(demo_basic, 2)\n",
      "  File \"mp.py\", line 57, in run_demo\n",
      "    mp.spawn(demo_fn,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/workspace/ai-school/mp.py\", line 38, in demo_basic\n",
      "    setup(rank, world_size)\n",
      "  File \"/workspace/ai-school/mp.py\", line 18, in setup\n",
      "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 577, in init_process_group\n",
      "    store, rank, world_size = next(rendezvous_iterator)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n",
      "    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n",
      "    return TCPStore(\n",
      "RuntimeError: Address already in use\n",
      "\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 8533) of binary: /opt/conda/bin/python\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:370: UserWarning: \n",
      "\n",
      "**********************************************************************\n",
      "               CHILD PROCESS FAILED WITH NO ERROR_FILE                \n",
      "**********************************************************************\n",
      "CHILD PROCESS FAILED WITH NO ERROR_FILE\n",
      "Child process 8533 (local_rank 0) FAILED (exitcode 1)\n",
      "Error msg: Process failed with exitcode 1\n",
      "Without writing an error file to <N/A>.\n",
      "While this DOES NOT affect the correctness of your application,\n",
      "no trace information about the error will be available for inspection.\n",
      "Consider decorating your top level entrypoint function with\n",
      "torch.distributed.elastic.multiprocessing.errors.record. Example:\n",
      "\n",
      "  from torch.distributed.elastic.multiprocessing.errors import record\n",
      "\n",
      "  @record\n",
      "  def trainer_main(args):\n",
      "     # do train\n",
      "**********************************************************************\n",
      "  warnings.warn(_no_error_file_warning_msg(rank, failure))\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==1.10.0a0+0aef44c', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 364, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 698, in main\n",
      "    run(args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 689, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 259, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "***************************************\n",
      "              mp.py FAILED             \n",
      "=======================================\n",
      "Root Cause:\n",
      "[0]:\n",
      "  time: 2022-02-18_16:04:50\n",
      "  rank: 0 (local_rank: 0)\n",
      "  exitcode: 1 (pid: 8533)\n",
      "  error_file: <N/A>\n",
      "  msg: \"Process failed with exitcode 1\"\n",
      "=======================================\n",
      "Other Failures:\n",
      "[1]:\n",
      "  time: 2022-02-18_16:04:50\n",
      "  rank: 1 (local_rank: 1)\n",
      "  exitcode: 1 (pid: 8534)\n",
      "  error_file: <N/A>\n",
      "  msg: \"Process failed with exitcode 1\"\n",
      "***************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 torchrun --standalone --nnodes=1 --nproc_per_node=2 mp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958db657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
