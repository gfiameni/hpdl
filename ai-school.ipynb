{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb7015a",
   "metadata": {},
   "source": [
    "# Lab Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74083927",
   "metadata": {},
   "source": [
    "# Preparing and launching a DDP application\n",
    "Independent of how a DDP application is launched, each process needs a mechanism to know its global and local ranks. Once this is known, all\n",
    "processes create a `ProcessGroup` that enables them to participate in collective communication operations such as AllReduce.\n",
    "\n",
    "PyTorch has relatively simple interface for distributed training. To do distributed training, the model would just have to be wrapped using DistributedDataParallel and the training script would just have to be launched using torch.distributed.launch or torchrun, or leave the code spawn multiple processes. \n",
    "\n",
    "This set of examples presents simple implementations of distributed training or message passings: \n",
    "1. CIFAR-10 classification using DistributedDataParallel wrapped ResNet models\n",
    "2. ToyModel using multiprocessing interface\n",
    "3. Message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80591919",
   "metadata": {},
   "source": [
    "## Example 1 (CIFAR-10 Classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a17379",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"resnet_ddp.py\"\"\"\n",
    "def main():\n",
    "\n",
    "    num_epochs_default = 5\n",
    "    batch_size_default = 256 # 1024\n",
    "    learning_rate_default = 0.1\n",
    "    random_seed_default = 0\n",
    "    model_dir_default = \"saved_models\"\n",
    "    model_filename_default = \"resnet_distributed.pth\"\n",
    "\n",
    "    # Each process runs on 1 GPU device specified by the local_rank argument.\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--local_rank\", type=int, help=\"Local rank. Necessary for using the torch.distributed.launch utility.\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, help=\"Number of training epochs.\", default=num_epochs_default)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Training batch size for one process.\", default=batch_size_default)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate.\", default=learning_rate_default)\n",
    "    parser.add_argument(\"--random_seed\", type=int, help=\"Random seed.\", default=random_seed_default)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=model_dir_default)\n",
    "    parser.add_argument(\"--model_filename\", type=str, help=\"Model filename.\", default=model_filename_default)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from saved checkpoint.\")\n",
    "    argv = parser.parse_args()\n",
    "\n",
    "    local_rank = argv.local_rank\n",
    "    num_epochs = argv.num_epochs\n",
    "    batch_size = argv.batch_size\n",
    "    learning_rate = argv.learning_rate\n",
    "    random_seed = argv.random_seed\n",
    "    model_dir = argv.model_dir\n",
    "    model_filename = argv.model_filename\n",
    "    resume = argv.resume\n",
    "\n",
    "    if local_rank is None:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # Create directories outside the PyTorch program\n",
    "    # Do not create directory here because it is not multiprocess safe\n",
    "    '''\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    '''\n",
    "\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "    # We need to use seeds to make sure that the models initialized in different processes are the same\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "    \n",
    "    # setup(local_rank, world_size)\n",
    "\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    # torch.distributed.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    # Encapsulate the model on the GPU assigned to the current process\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "    model = model.to(device)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    # We only save the model who uses device \"cuda:0\"\n",
    "    # To resume, the device for the saved model would also be \"cuda:0\"\n",
    "    if resume == True:\n",
    "        map_location = {\"cuda:0\": \"cuda:{}\".format(local_rank)}\n",
    "        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Data should be prefetched\n",
    "    # Download should be set to be False, because it is not multiprocess safe\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=True, download=True, transform=transform) \n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Restricts data loading to a subset of the dataset exclusive to the current process\n",
    "    train_sampler = DistributedSampler(dataset=train_set)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
    "    # Test loader does not have to follow distributed sampling strategy\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    # Loop over the dataset multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t0 = time.time()\n",
    "        # Save and evaluate model routinely\n",
    "        if epoch % 1 == 0:\n",
    "            if local_rank == 0:\n",
    "                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)\n",
    "                torch.save(ddp_model.state_dict(), model_filepath)\n",
    "                print(\"-\" * 75)\n",
    "                print(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "                print(\"-\" * 75)\n",
    "\n",
    "        ddp_model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Local Rank: {}, Epoch: {}, Training ...\".format(local_rank, epoch))\n",
    "        print(\"Time {} seconds\".format(round(time.time() - t0, 2)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b0a21",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "- Use --local_rank for argparse if we are going to use torch.distributed.launch to launch distributed training.\n",
    "- Set random seed to make sure that the models initialized in different processes are the same. \n",
    "- Use DistributedDataParallel to wrap the model for distributed training.\n",
    "- Use DistributedSampler to training data loader.\n",
    "- To save models, each node would save a copy of the checkpoint file in the local hard drive.\n",
    "- Downloading dataset and making directories should be avoided in the distributed training program as they are not multi-process safe, unless we use some sort of barriers, such as torch.distributed.barrier.\n",
    "- The node communication bandwidth are extremely important for multi-node distributed training. Instead of randomly finding two computers in the network, try to use the nodes from the specialized computing clusters, since the communications between the nodes are highly optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4490e59",
   "metadata": {},
   "source": [
    "### Get familiar with the system architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a323656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\u001b[0m\n",
      "GPU0\t X \tNV1\tNV1\tNV2\t0-39\n",
      "GPU1\tNV1\t X \tNV2\tNV1\t0-39\n",
      "GPU2\tNV1\tNV2\t X \tNV1\t0-39\n",
      "GPU3\tNV2\tNV1\tNV1\t X \t0-39\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing a single PCIe switch\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead8854",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60a5b9",
   "metadata": {},
   "source": [
    "### Distributed launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b93aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "usage: launch.py [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                 [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                 [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                 [--max_restarts MAX_RESTARTS]\n",
      "                 [--monitor_interval MONITOR_INTERVAL]\n",
      "                 [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                 [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                 [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                 [--master_port MASTER_PORT] [--use_env]\n",
      "                 training_script ...\n",
      "launch.py: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "! python -m torch.distributed.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370cca88",
   "metadata": {},
   "source": [
    "#### Launch command 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a6a6723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 0, Accuracy: 0.0\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 0, Training ...\n",
      "Time 6.9 seconds\n",
      "Local Rank: 1, Epoch: 0, Training ...\n",
      "Time 6.91 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 1, Accuracy: 0.3024\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 1, Training ...\n",
      "Time 5.02 seconds\n",
      "Local Rank: 1, Epoch: 1, Training ...\n",
      "Time 5.04 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 2, Accuracy: 0.4029\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 2, Training ...\n",
      "Time 5.2 seconds\n",
      "Local Rank: 1, Epoch: 2, Training ...\n",
      "Time 5.19 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 3, Accuracy: 0.4441\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 3, Training ...\n",
      "Time 4.88 seconds\n",
      "Local Rank: 1, Epoch: 3, Training ...\n",
      "Time 4.88 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 4, Accuracy: 0.5027\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 4, Training ...\n",
      "Time 5.11 seconds\n",
      "Local Rank: 1, Epoch: 4, Training ...\n",
      "Time 5.12 seconds\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=2,3 python -W ignore -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=1234 resnet_ddp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3f7b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 24 11:40:22 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   53C    P0   221W / 300W |  28047MiB / 32478MiB |     94%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    40W / 300W |     12MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    59W / 300W |   2037MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    40W / 300W |     12MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0de5bf",
   "metadata": {},
   "source": [
    "### Torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "! torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc992b",
   "metadata": {},
   "source": [
    "#### Launch command 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nnodes=1 --nproc_per_node=2 resnet_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825976b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094b9c0",
   "metadata": {},
   "source": [
    "## Example 2 (ToyModel using multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0732f8",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"mp.py\"\"\"\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef195e31",
   "metadata": {},
   "source": [
    "#### Launch command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 python -W ignore mp.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd35c0",
   "metadata": {},
   "source": [
    "**What is wrong with the following command?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 torchrun --standalone --nnodes=1 --nproc_per_node=2 mp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c690c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ada4ee",
   "metadata": {},
   "source": [
    "## Example 3 (Message passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ddbfa",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"send_receive.py\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "\n",
    "def init_processes(rank, size, fn, backend='tcp'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run_non_blocking)) # Options: run_non_blocking, run_blocking, run_allreduce\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "```\n",
    "\n",
    "The above script spawns two processes who will each setup the distributed environment, initialize the process group (`dist.init_process_group`), and finally execute the given `run` function. \n",
    "\n",
    "The `init_processes` ensures that every process will be able to coordinate through a master, using the same ip address and port. Note that we used the TCP backend, but we could have used [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) or [Gloo](http://github.com/facebookincubator/gloo) instead. \n",
    "\n",
    "### Point-to-Point Communication\n",
    "<!--\n",
    "* send/recv\n",
    "* isend/irecv\n",
    "-->\n",
    "\n",
    "![alt text](./figs/send_recv.png)\n",
    "\n",
    "\n",
    "A transfer of data from one process to another is called a point-to-point communication. These are achieved through the `send` and `recv` functions or their *immediate* counter-parts, `isend` and `irecv`.\n",
    "\n",
    "```python\n",
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "\n",
    "In the above example, both processes start with a zero tensor, then process 0 increments the tensor and sends it to process 1 so that they both end up with 1.0. Notice that process 1 needs to allocate memory in order to store the data it will receive.\n",
    "\n",
    "Also notice that `send`/`recv` are **blocking**: both processes stop until the communication is completed. On the other hand immediates are **non-blocking**; the script continues its execution and the methods return a `DistributedRequest` object upon which we can choose to `wait()`.\n",
    "\n",
    "```python\n",
    "\"\"\"Non-blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_non_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        req = dist.isend(tensor=tensor, dst=1)\n",
    "        print('Rank 0 started sending')\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print('Rank 1 started receiving')\n",
    "        print('Rank 1 has data ', tensor[0])\n",
    "    req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "\n",
    "Running the above function might result in process 1 still having 0.0 while having already started receiving. However, after `req.wait()` has been executed we are guaranteed that the communication took place, and that the value stored in `tensor[0]` is 1.0.\n",
    "\n",
    "Point-to-point communication is useful when we want a fine-grained control over the communication of our processes. They can be used to implement fancy algorithms, such as the one used in [Baidu's DeepSpeech](https://github.com/baidu-research/baidu-allreduce) or [Facebook's large-scale experiments](https://research.fb.com/publications/imagenet1kin1h/).)\n",
    "\n",
    "### Collective Communication\n",
    "<!--\n",
    "* gather\n",
    "* reduce\n",
    "* broadcast\n",
    "* scatter\n",
    "* all_reduce\n",
    "-->\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/scatter.png' width=100% /><br/>\n",
    "<b>Broadcast</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/all_gather.png' width=100% /><br/>\n",
    "<b>AllGather</b>\n",
    "</td>\n",
    "\n",
    "</tr><tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/reduce.png' width=100% /><br/>\n",
    "<b>Reduce</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/all_reduce.png' width=100% /><br/>\n",
    "<b>AllReduce</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/scatter.png' width=100% /><br/>\n",
    "<b>Scatter</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/gather.png' width=100% /><br/>\n",
    "<b>Gather</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a **group**. A group is a subset of all our processes. To create a group, we can pass a list of ranks to `dist.new_group(group)`. By default, collectives are executed on the all processes, also known as the **world**. For example, in order to obtain the sum of all tensors at all processes, we can use the `dist.all_reduce(tensor, op, group)` collective.\n",
    "\n",
    "```python\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "def run_all_reduce(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    group = dist.new_group([0, 1]) \n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "```\n",
    "\n",
    "Since we want the sum of all tensors in the group, we use `dist.reduce_op.SUM` as the reduce operator. Generally speaking, any commutative mathematical operation can be used as an operator. Out-of-the-box, PyTorch comes with 4 such operators, all working at the element-wise level:\n",
    "\n",
    "* `dist.reduce_op.SUM`,\n",
    "* `dist.reduce_op.PRODUCT`,\n",
    "* `dist.reduce_op.MAX`,\n",
    "* `dist.reduce_op.MIN`.\n",
    "\n",
    "In addition to `dist.all_reduce(tensor, op, group)`, there are a total of 6 collectives currently implemented in PyTorch.\n",
    "\n",
    "* `dist.broadcast(tensor, src, group)`: Copies `tensor` from `src` to all other processes.\n",
    "* `dist.reduce(tensor, dst, op, group)`: Applies `op` to all `tensor` and stores the result in `dst`.\n",
    "* `dist.all_reduce(tensor, op, group)`: Same as reduce, but the result is stored in all processes.\n",
    "* `dist.scatter(tensor, src, scatter_list, group)`: Copies the $i^{\\text{th}}$ tensor `scatter_list[i]` to the $i^{\\text{th}}$ process.\n",
    "* `dist.gather(tensor, dst, gather_list, group)`: Copies `tensor` from all processes in `dst`.\n",
    "* `dist.all_gather(tensor_list, tensor, group)`: Copies `tensor` from all processes to `tensor_list`, on all processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f0a4b",
   "metadata": {},
   "source": [
    "#### Launch command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05dd800",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python send_receive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a8699",
   "metadata": {},
   "source": [
    "## Credits\n",
    "- [https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)\n",
    "- [https://leimao.github.io/blog/PyTorch-Distributed-Training/](https://leimao.github.io/blog/PyTorch-Distributed-Training/)\n",
    "- [https://pytorch.org/tutorials/intermediate/ddp_tutorial.html](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
    "- [https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "- [https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)\n",
    "- [https://huggingface.co/docs/transformers/performance](https://huggingface.co/docs/transformers/performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d4cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
