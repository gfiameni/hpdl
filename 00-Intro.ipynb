{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e41041",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edd9af",
   "metadata": {},
   "source": [
    "The limited computation resource might discourage distibuted training across multiple gpus. It’s basically an easy job to wrap the model with DDP (short for DistributedDataParallel). \n",
    "\n",
    "In this repo, I want to share code, insighs with all beginners in DDP. I’m not going to include detailed explanation of how DDP works, instead, I provide minimum knowledge needed to make the model run in multiple gpus. Note that I only introduce DDP on one machine with multiple gpus, which is the most general case (Otherwise, we should use model parallel as stated in the official [blog](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce356f2",
   "metadata": {},
   "source": [
    "1. [Intro - notes about pytorch ddp]()\n",
    "1. [Data Parallelism](02-Data_Parallelism.ipynb)\n",
    "1. [Data Parallel v2](02-Data_Parallelism_2.ipynb)\n",
    "1. [Model Parallel](03-Model_Parallel.ipynb)\n",
    "1. [Pipeline Parallelism]()\n",
    "1. [ZeRO](04-ZeRO.ipynb)\n",
    "1. [Memory]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0bdfd",
   "metadata": {},
   "source": [
    "### Get familiar with the system architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b794db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\u001b[0m\n",
      "GPU0\t X \tNV1\tNV1\tNV2\t0-39\n",
      "GPU1\tNV1\t X \tNV2\tNV1\t0-39\n",
      "GPU2\tNV1\tNV2\t X \tNV1\t0-39\n",
      "GPU3\tNV2\tNV1\tNV1\t X \t0-39\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing a single PCIe switch\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228a065",
   "metadata": {},
   "source": [
    "### Distributed launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa69a00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "usage: launch.py [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                 [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                 [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                 [--max_restarts MAX_RESTARTS]\n",
      "                 [--monitor_interval MONITOR_INTERVAL]\n",
      "                 [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                 [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                 [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                 [--master_port MASTER_PORT] [--use_env]\n",
      "                 training_script ...\n",
      "launch.py: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "! python -m torch.distributed.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d251da",
   "metadata": {},
   "source": [
    "### Torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f7912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torchrun [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                [--max_restarts MAX_RESTARTS]\n",
      "                [--monitor_interval MONITOR_INTERVAL]\n",
      "                [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                [--master_port MASTER_PORT]\n",
      "                training_script ...\n",
      "torchrun: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "! torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fa9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
