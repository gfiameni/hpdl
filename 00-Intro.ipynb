{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83941f71",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb13a29",
   "metadata": {},
   "source": [
    "Modern deep neural networks and the size of training data are becoming extremely large. Training new DNN models on a single GPU node is becoming more and more difficult. Thus, we need to build a large distributed machine learning system, which can use large-scale clusters to train models with big data and achieve equivalent or even better performance than a single node. To realize this goal, we need to optimize the parallelism strategy of machine learning systems for higher speed-up and more efficient training. \n",
    "\n",
    "Related work in this area can be divided into the following categories: Model Parallelism, Data Parallelism, and Hybrid Parallelism.\n",
    "\n",
    "- Data parallelism is the most widely used strategy. It is used for scenarios where the size of training data is large and cannot put into a single machine. To solve this problem, data parallelism allows us to divide the data into multiple shards and distribute them to different nodes. Each node first uses local data which has a small size to train a sub-model, and communicate with other nodes to ensure that the training results from each node can be integrated at certain times, and finally obtain the global model. The parameter update policy (SGD for machine learning/deep learning) for data parallelism can be divided into two categories: asynchronous update and synchronous update. The disadvantages of data parallelism are also obvious. Since each sub-model needs to submit the gradient after each iteration of training, the network communication overhead is very large.\n",
    "\n",
    "- Model parallelism is used for scenarios where the size of the model is very large and cannot be stored in local memory. In this case, we need to split the model into different modules (e.g., different layers in DNN). Then, each module can be put into different nodes for training. At this time, frequent inter-node communication between different nodes may be required. The performance of model parallelism depends on two aspects, connectivity structure and compute demand of operations. Although model parallelism can solve the problem of large model training, it will also bring us low network traffic and increase training time.\n",
    "\n",
    "- Hybrid parallelism is the combination of data parallelism and model parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59272c6a",
   "metadata": {},
   "source": [
    "1. [Data Parallelism](01-Data_Parallelism.ipynb)\n",
    "1. [Model Parallelism](02-Model_Parallelism.ipynb)\n",
    "1. [Message Passing](07-Message_Passing.ipynb)\n",
    "1. [Horovod](08-Horovod.ipynb)\n",
    "1. [Mixed Precision](06-DDP_Mixed_Precision.ipynb)\n",
    "1. [Memory Format](05-Memory_Format.ipynb)\n",
    "1. [Pipeline Parallelism](03-Pipeline_Parallelism.ipynb)\n",
    "1. [ZeRO](04-ZeRO.ipynb)\n",
    "1. [PyTorch SLLURM](https://github.com/gfiameni/nvdoc-italy) Working in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db1c04",
   "metadata": {},
   "source": [
    "# Application process topologies\n",
    "A Distributed Data Parallel (DDP) application can be executed on multiple nodes where each node can consist of multiple GPU devices. Each node in turn can run multiple copies of the DDP application, each of which processes its models on multiple GPUs.\n",
    "\n",
    "Let _N_ be the number of nodes on which the application is running and _G_ be the number of GPUs per node. The total number of application\n",
    "processes running across all the nodes at one time is called the **World Size**, _W_ and the number of processes running on each node\n",
    "is referred to as the **Local World Size**, _L_.\n",
    "\n",
    "Each application process is assigned two IDs: a _local_ rank in \\[0,_L_-1\\] and a _global_ rank in \\[0, _W_-1\\].\n",
    "\n",
    "To illustrate the terminology defined above, consider the case where a DDP application is launched on two nodes, each of which has four GPUs. We would then like each process to span two GPUs each. The mapping of processes to nodes is shown in the figure below:\n",
    "\n",
    "![ProcessMapping](https://user-images.githubusercontent.com/875518/77676984-4c81e400-6f4c-11ea-87d8-f2ff505a99da.png)\n",
    "\n",
    "While there are quite a few ways to map processes to nodes, a good rule of thumb is to have one process span a single GPU. This enables the DDP application to have as many parallel reader streams as there are GPUs and in practice provides a good balance between I/O and computational costs. In the rest of this tutorial, we assume that the application follows this heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a21b7",
   "metadata": {},
   "source": [
    "### Get familiar with the system architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a75869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\u001b[0m\n",
      "GPU0\t X \tNV1\tNV1\tNV2\t0-39\n",
      "GPU1\tNV1\t X \tNV2\tNV1\t0-39\n",
      "GPU2\tNV1\tNV2\t X \tNV1\t0-39\n",
      "GPU3\tNV2\tNV1\tNV1\t X \t0-39\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing a single PCIe switch\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb55b35",
   "metadata": {},
   "source": [
    "### Distributed launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7140ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "usage: launch.py [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                 [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                 [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                 [--max_restarts MAX_RESTARTS]\n",
      "                 [--monitor_interval MONITOR_INTERVAL]\n",
      "                 [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                 [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                 [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                 [--master_port MASTER_PORT] [--use_env]\n",
      "                 training_script ...\n",
      "launch.py: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "! python -m torch.distributed.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f96e16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4e02115",
   "metadata": {},
   "source": [
    "### Torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e97cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torchrun [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                [--max_restarts MAX_RESTARTS]\n",
      "                [--monitor_interval MONITOR_INTERVAL]\n",
      "                [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                [--master_port MASTER_PORT]\n",
      "                training_script ...\n",
      "torchrun: error: the following arguments are required: training_script, training_script_args\n"
     ]
    }
   ],
   "source": [
    "! torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045b1cb",
   "metadata": {},
   "source": [
    "## Hot to run the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a161b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add reference to NGC container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee43730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
