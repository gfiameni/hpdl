{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafaf079",
   "metadata": {},
   "source": [
    "# Overview of Pytorch Distributed Data Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffbc1a8",
   "metadata": {},
   "source": [
    "Ref: (https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e14d1",
   "metadata": {},
   "source": [
    "This tutorial is organized as:\n",
    "- Overview of DDP\n",
    "- Implementation of DDP workflow (Steps 1–6)\n",
    "- Issues about `dist.barrier()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5739a",
   "metadata": {},
   "source": [
    "## Overview of DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0584df",
   "metadata": {},
   "source": [
    "Terms used in distributed training:\n",
    "\n",
    "- **master node**: the main GPU responsible for synchronizations, making copies, loading models, writing checpoints and logs;\n",
    "- **process group**: if you want to train/test the model over K GPUs, then the K process forms a group, which is supported by a backend (pytorch managed that for you, according to the [documentation](https://pytorch.org/docs/1.9.0/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel), nccl is the most recommended backend);\n",
    "- **rank**: within the process group, each process is identified by its rank, from 0 to K-1;\n",
    "- **world size**: the number of processes in the group.\n",
    "\n",
    "Pytorch provides two settings for distributed training: `torch.nn.DataParallel` (DP) and `torch.nn.parallel.DistributedDataParallel` (DDP), where the latter is officially recommended. In short, DDP is faster, more flexible than DP. The fundamental thing DDP does is to copy the model to multiple gpus, gather the gradients from them, average the gradients to update the model, then synchronize the model over all K processes. \n",
    "\n",
    "We can also gather/scatter tensors/objects other than gradients by torch.distributed.gather/scatter/reduce.\n",
    "\n",
    "In case the model can fit on one GPU (it can be trained on one GPU with batch_size = 1) and we want to train/test it on K GPUs, the best practice of DDP is to copy the model onto the K GPUs (the DDP class automatically does this for you) and split the dataloader to K non-overlapping groups to feed into K models respectively.\n",
    "\n",
    "We have to do the following things:\n",
    "\n",
    "1. setup the process group, which is three lines of code and needs no modification;\n",
    "2. split the dataloader to each process in the group, which can be easily achieved by torch.utils.data.DistributedSampler or any customized sampler;\n",
    "3. wrap our model with DDP, which is one line of code and barely needs modification;\n",
    "4. train/test our model, which is the same as is on 1 GPU;\n",
    "5. clean up the process groups, which is one line of code;\n",
    "6. optional: gather extra data among processes (possibly needed for distributed testing), which is basically one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab6fef",
   "metadata": {},
   "source": [
    "## Setup the process group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181527a",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.distributed as dist\n",
    "\n",
    "def setup(rank, world_size):    \n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'    \n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60837b31",
   "metadata": {},
   "source": [
    "## Split the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45982e33",
   "metadata": {},
   "source": [
    "We can easily split our dataloader by `torch.utils.data.distributed.DistributedSampler`. The sampler returns an iterator over indices, which are fed into dataloader to bachify.\n",
    "\n",
    "The DistributedSampler split the total indices of the dataset into `world_size` parts, and evenly distributes them to the dataloader in each process without duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed4921",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def prepare(rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n",
    "    dataset = Your_Dataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, \n",
    "                            drop_last=False, shuffle=False, sampler=sampler)\n",
    "    return dataloader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9770850c",
   "metadata": {},
   "source": [
    "Suppose K=3, and the length of dataset is 10. We must understand that DistributedSampler imposes even partition of indices.\n",
    "\n",
    "If we set `drop_last=False` when defining `DistributedSampler`, it will automatically pad. For example, it splits indices `[0,1,2,3,4,5,6,7,8,9]` to `[0,3,6,9]` when `rank=1`, `[0,4,7,0]` when `rank=2`, and `[2,5,8,0]` when `rank=3`. As you can see, such padding may cause issues because the padded 0 is a data record. Otherwise, it will strip off the trailing elements. For example, it splits the indices to `[0,3,6]` at `rank=1`, `[1,4,7]` at `rank=2`, and `[2,5,8]` at `rank=3`. In this case, it tailored 9 to make the indice number divisible by `world_size`.\n",
    "\n",
    "It is very simple to customize our `Sampler`. We only need to create a class, then define its `__iter__()` and `__len__()` function. Refer to the [official documentation](https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler) for more details.\n",
    "\n",
    "BTW, you’d better set the `num_workers=0` when distributed training, because creating extra threads in the children processes may be problemistic. I also found `pin_memory=False` avoids many horrible bugs, maybe such things are machine-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16da46",
   "metadata": {},
   "source": [
    "## Wrap the model with DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd68f95",
   "metadata": {},
   "source": [
    "We should first move our model to the specific GPU (recall that one model replica resides in one GPU), then we wrap it with DDP class. The following function takes in an argument rank, which we will introduce soon. For now, we just keep in mind rank equals the GPU id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f3ac8",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def main(rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)    \n",
    "    \n",
    "    # prepare the dataloader\n",
    "    dataloader = prepare(rank, world_size)\n",
    "    \n",
    "    # instantiate the model(it's your own model) and move it to the right device\n",
    "    model = Model().to(rank)\n",
    "    \n",
    "    # wrap the model with DDP\n",
    "    # device_ids tell DDP where is your model\n",
    "    # output_device tells DDP where to output, in our case, it is rank\n",
    "    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model    \n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a121fec",
   "metadata": {},
   "source": [
    "There are a few tricky things here:\n",
    "\n",
    "- When we want to access some customized attributes of the DDP wrapped model, we must reference `model.module`. That is to say, our model instance is saved as a module attribute of the DDP model. If we assign some attributes `xxx` other than built-in properties or functions, we must access them by `model.module.xxx`.\n",
    "- When we save the DDP model, our state_dict would add a module prefix to all parameters. \n",
    "- Consequently, if we want to load a DDP saved model to a non-DDP model, we have to manually strip the extra prefix. I provide my code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf8409",
   "metadata": {},
   "source": [
    "In case we load a DDP model checkpoint to a non-DDP modelmodel_dict = OrderedDict()\n",
    "\n",
    "```python\n",
    "import re\n",
    "pattern = re.compile('module.')\n",
    "\n",
    "for k,v in state_dict.items():\n",
    "    if re.search(\"module\", k):\n",
    "        model_dict[re.sub(pattern, '', k)] = v\n",
    "    else:\n",
    "        model_dict = state_dict\n",
    "        \n",
    "model.load_state_dict(model_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee60b5",
   "metadata": {},
   "source": [
    "## Train/test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b4b45",
   "metadata": {},
   "source": [
    "This part is the key to implementing DDP. First we need to know the basis of multi-processing: all children processes together with the parent process run the same code.\n",
    "\n",
    "In PyTorch, `torch.multiprocessing` provides convenient ways to create parallel processes. As the official documentation says,\n",
    "\n",
    "> The spawn function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them.\n",
    "\n",
    "So, using `spawn` is a good choice.\n",
    "\n",
    "In our script, we should define a train/test function before spawning it to parallel processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3a247",
   "metadata": {},
   "source": [
    "```python\n",
    "def main(rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)    # prepare the dataloader\n",
    "    dataloader = prepare(rank, world_size)\n",
    "    \n",
    "    # instantiate the model(it's your own model) and move it to the right device\n",
    "    model = Your_Model().to(rank)\n",
    "    \n",
    "    # wrap the model with DDP\n",
    "    # device_ids tell DDP where is your model\n",
    "    # output_device tells DDP where to output, in our case, it is rank\n",
    "    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model    \n",
    "    \n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)    \n",
    "    \n",
    "    #################### The above is defined previously\n",
    "   \n",
    "    optimizer = Your_Optimizer()\n",
    "    loss_fn = Your_Loss()    \n",
    "    for epoch in epochs:\n",
    "        # if we are using DistributedSampler, we have to tell it which epoch this is\n",
    "        dataloader.sampler.set_epoch(epoch)       \n",
    "        \n",
    "        for step, x in enumerate(dataloader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            pred = model(x)\n",
    "            label = x['label']\n",
    "            \n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "    cleanup()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eeb7b5",
   "metadata": {},
   "source": [
    "This `main` function is run in every parallel process. We now need to call it by `spawn` method. In our `.py` script, we write:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bd191",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "world_size = 2\n",
    "mp.spawn(\n",
    "    main,\n",
    "    args=(world_size),\n",
    "    nprocs=world_size\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e65d79",
   "metadata": {},
   "source": [
    "Remember the first argument of `main` is `rank`? It is automatically passed to each process by `mp.spawn`, we don’t need to pass it explicitly. `rank=0` is the master node by default. The `rank` ranges from `0` to `K-1` (2 in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a519a9",
   "metadata": {},
   "source": [
    "## Clean up the process groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dbded",
   "metadata": {},
   "source": [
    "The last line of main function is the clean up function, which is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187c9d8",
   "metadata": {},
   "source": [
    "```python\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f66e90",
   "metadata": {},
   "source": [
    "## Optional: Gather extra data among processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da044d",
   "metadata": {},
   "source": [
    "Sometimes we need to collect some data from all processes, such as the testing result. We can easily gather tensors by `dist.all_gather` and objects by `dist.all_gather_object`.\n",
    "\n",
    "Without loss of generality, I assume we want to collect python objects. The only constraint of the object is it must be serializable, which is basically everything in python. One should always assign `torch.cuda.set_device(rank)` before using `all_gather_xxx`. And, if we want to store a tensor in the object, it must locate at the `output_device`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42486922",
   "metadata": {},
   "source": [
    "```python\n",
    "def main(rank, world_size):\n",
    "    torch.cuda.set_device(rank)\n",
    "    data = {\n",
    "        'tensor': torch.ones(3,device=rank) + rank,\n",
    "        'list': [1,2,3] + rank,\n",
    "        'dict': {'rank':rank}   \n",
    "    }\n",
    "    \n",
    "    # we have to create enough room to store the collected objects\n",
    "    outputs = [None for _ in range(world_size)]\n",
    "    \n",
    "    # the first argument is the collected lists, the second argument is the data unique in each process\n",
    "    dist.all_gather_object(outputs, data)    \n",
    "    \n",
    "    # we only want to operate on the collected objects at master node\n",
    "    if rank == 0:\n",
    "        print(outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2fc7c6",
   "metadata": {},
   "source": [
    "## Issues about dist.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcfe7d",
   "metadata": {},
   "source": [
    "The most confusing thing to me is when to use `dist.barrier()`. As the documentation says, it synchronizes processes. In other words, it blocks processes until all of them reaches the same line of code: `dist.barrier()`. I summarize its usage as follows:\n",
    "\n",
    "- we do not need it when training, since DDP automatically does it for us (in `loss.backward()`);\n",
    "- we do not need it when gathering data, since `dist.all_gather_object` does it for us;\n",
    "- we need it when enforcing execution order of codes, [say one process loads the model that another process saves](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) (I can hardly imagine this scenario is needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a67b0e",
   "metadata": {},
   "source": [
    "# Preparing and launching a DDP application\n",
    "Independent of how a DDP application is launched, each process needs a mechanism to know its global and local ranks. Once this is known, all\n",
    "processes create a `ProcessGroup` that enables them to participate in collective communication operations such as AllReduce.\n",
    "\n",
    "PyTorch has relatively simple interface for distributed training. To do distributed training, the model would just have to be wrapped using DistributedDataParallel and the training script would just have to be launched using torch.distributed.launch or torchrun, or leave the code spawn multiple processes. \n",
    "\n",
    "This set of examples presents simple implementations of distributed training or message passings: \n",
    "1. CIFAR-10 classification using DistributedDataParallel wrapped ResNet models\n",
    "2. ToyModel using multiprocessing interface\n",
    "3. Message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7430d",
   "metadata": {},
   "source": [
    "## Example 1 (Image classification using ResNet-18 over CIFAR-10)\n",
    "\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/ResNet-18-Architecture.png' width=100% /><br/>\n",
    "<b>ResNet-18</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555455f",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"ddp.py\"\"\"\n",
    "def main():\n",
    "\n",
    "    num_epochs_default = 5\n",
    "    batch_size_default = 256 # 1024\n",
    "    learning_rate_default = 0.1\n",
    "    random_seed_default = 0\n",
    "    model_dir_default = \"saved_models\"\n",
    "    model_filename_default = \"resnet_distributed.pth\"\n",
    "\n",
    "    # Each process runs on 1 GPU device specified by the local_rank argument.\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--local_rank\", type=int, help=\"Local rank. Necessary for using the torch.distributed.launch utility.\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, help=\"Number of training epochs.\", default=num_epochs_default)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Training batch size for one process.\", default=batch_size_default)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate.\", default=learning_rate_default)\n",
    "    parser.add_argument(\"--random_seed\", type=int, help=\"Random seed.\", default=random_seed_default)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=model_dir_default)\n",
    "    parser.add_argument(\"--model_filename\", type=str, help=\"Model filename.\", default=model_filename_default)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from saved checkpoint.\")\n",
    "    argv = parser.parse_args()\n",
    "\n",
    "    local_rank = argv.local_rank\n",
    "    num_epochs = argv.num_epochs\n",
    "    batch_size = argv.batch_size\n",
    "    learning_rate = argv.learning_rate\n",
    "    random_seed = argv.random_seed\n",
    "    model_dir = argv.model_dir\n",
    "    model_filename = argv.model_filename\n",
    "    resume = argv.resume\n",
    "\n",
    "    if local_rank is None:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # Create directories outside the PyTorch program\n",
    "    # Do not create directory here because it is not multiprocess safe\n",
    "    '''\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    '''\n",
    "\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "    # We need to use seeds to make sure that the models initialized in different processes are the same\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    # torch.distributed.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    # Encapsulate the model on the GPU assigned to the current process\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "    model = model.to(device)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    # We only save the model who uses device \"cuda:0\"\n",
    "    # To resume, the device for the saved model would also be \"cuda:0\"\n",
    "    if resume == True:\n",
    "        map_location = {\"cuda:0\": \"cuda:{}\".format(local_rank)}\n",
    "        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Data should be prefetched\n",
    "    # Download should be set to be False, because it is not multiprocess safe\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=True, download=True, transform=transform) \n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Restricts data loading to a subset of the dataset exclusive to the current process\n",
    "    train_sampler = DistributedSampler(dataset=train_set)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
    "    # Test loader does not have to follow distributed sampling strategy\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    # Loop over the dataset multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t0 = time.time()\n",
    "        # Save and evaluate model routinely\n",
    "        if epoch % 1 == 0:\n",
    "            if local_rank == 0:\n",
    "                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)\n",
    "                torch.save(ddp_model.state_dict(), model_filepath)\n",
    "                print(\"-\" * 75)\n",
    "                print(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "                print(\"-\" * 75)\n",
    "\n",
    "        ddp_model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Local Rank: {}, Epoch: {}, Training ...\".format(local_rank, epoch))\n",
    "        print(\"Time {} seconds\".format(round(time.time() - t0, 2)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1529f00",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "- Use --local_rank for argparse if we are going to use torch.distributed.launch to launch distributed training.\n",
    "- Set random seed to make sure that the models initialized in different processes are the same. \n",
    "- Use DistributedDataParallel to wrap the model for distributed training.\n",
    "- Use DistributedSampler to training data loader.\n",
    "- To save models, each node would save a copy of the checkpoint file in the local hard drive.\n",
    "- Downloading dataset and making directories should be avoided in the distributed training program as they are not multi-process safe, unless we use some sort of barriers, such as torch.distributed.barrier.\n",
    "- The node communication bandwidth are extremely important for multi-node distributed training. Instead of randomly finding two computers in the network, try to use the nodes from the specialized computing clusters, since the communications between the nodes are highly optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858f8bd",
   "metadata": {},
   "source": [
    "#### Launch command 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e585a955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Max memory allocated after creating local model: 46.0MB \n",
      "Max memory allocated after creating DDP: 94.0MB \n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 0, Accuracy: 0.0\n",
      "---------------------------------------------------------------------------\n",
      "Max memory allocated before optimizer step(): 212.0MB \n",
      "Max memory allocated after optimizer step(): 212.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "Max memory allocated before optimizer step(): 291.0MB \n",
      "Max memory allocated after optimizer step(): 291.0MB \n",
      "^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16637 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 16638 closing signal SIGINT\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f20e7a26310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7db70a0310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1301, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1301, in _shutdown_workers\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "Traceback (most recent call last):\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "  File \"ddp.py\", line 240, in <module>\n",
      "    main()\n",
      "  File \"ddp.py\", line 221, in main\n",
      "    outputs = ddp_model(inputs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 884, in forward\n",
      "    output = self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 249, in forward\n",
      "    return self._forward_impl(x)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 237, in _forward_impl\n",
      "    x = self.layer1(x)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "Traceback (most recent call last):\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "  File \"ddp.py\", line 240, in <module>\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 81, in forward\n",
      "    out = self.relu(out)\n",
      "KeyboardInterrupt\n",
      "    main()\n",
      "  File \"ddp.py\", line 221, in main\n",
      "    outputs = ddp_model(inputs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 884, in forward\n",
      "    output = self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 249, in forward\n",
      "    return self._forward_impl(x)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 239, in _forward_impl\n",
      "    x = self.layer3(x)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 78, in forward\n",
      "    identity = self.downsample(x)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 442, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=2,3 python -W ignore -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=1234 ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d111b",
   "metadata": {},
   "source": [
    "#### Launch command 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d101cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Max memory allocated after creating local model: 46.0MB \n",
      "^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17707 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17708 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nnodes=1 --nproc_per_node=2 ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d21af",
   "metadata": {},
   "source": [
    "## Credits\n",
    "- [https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)\n",
    "- [https://leimao.github.io/blog/PyTorch-Distributed-Training/](https://leimao.github.io/blog/PyTorch-Distributed-Training/)\n",
    "- [https://pytorch.org/tutorials/intermediate/ddp_tutorial.html](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
    "- [https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "- [https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)\n",
    "- [https://huggingface.co/docs/transformers/performance](https://huggingface.co/docs/transformers/performance)\n",
    "- [https://github.com/pytorch/tutorials/blob/master/intermediate_source/dist_tuto.rst](https://github.com/pytorch/tutorials/blob/master/intermediate_source/dist_tuto.rst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
