{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03836ee",
   "metadata": {},
   "source": [
    "# Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130ab38",
   "metadata": {},
   "source": [
    "This notebook is organized as follows:\n",
    "- Overview of Distributed Data Parallel (DDP)\n",
    "- Implementation of DDP workflow (Steps 1â€“6)\n",
    "- Issues about `dist.barrier()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68883d0a",
   "metadata": {},
   "source": [
    "## Overview of Pytorch Distributed Data Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3a89b",
   "metadata": {},
   "source": [
    "Terms used in distributed training:\n",
    "\n",
    "- **master node**: the main GPU responsible for synchronizations, making copies, loading models, writing checkpoints and logs;\n",
    "- **process group**: if you want to train/test the model over K GPUs, then the K process forms a group, which is supported by a backend (PyTorch managed that for you, according to the [documentation](https://pytorch.org/docs/1.9.0/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel), NCCL is the most recommended one);\n",
    "- **rank**: within the process group, each process is identified by its rank, from 0 to K-1;\n",
    "- **world size**: the number of processes in the group.\n",
    "\n",
    "PyTorch provides two settings for distributed training: `torch.nn.DataParallel` (DP) and `torch.nn.parallel.DistributedDataParallel` (DDP), where the latter is officially recommended. In short, DDP is faster, more flexible than DP. **The fundamental thing DDP does is to copy the model to multiple gpus, gather the gradients from them, average the gradients to update the model, then synchronize the model over all K processes**. \n",
    "\n",
    "We can also gather/scatter tensors/objects other than gradients by torch.distributed.gather/scatter/reduce.\n",
    "\n",
    "In case the model can fit on one GPU (it can be trained on one GPU with batch_size = 1) and we want to train/test it on K GPUs, the best practice of DDP is to copy the model onto the K GPUs (the DDP class automatically does this for you) and split the dataloader to K non-overlapping groups to feed into K models respectively.\n",
    "\n",
    "In order to make your model leverages multiple GPUS, this is the list of steps to follow:\n",
    "\n",
    "1. setup the process group, which is three lines of code and needs no modification;\n",
    "2. split the dataloader to each process in the group, which can be easily achieved by `torch.utils.data.DistributedSampler` or any customized sampler;\n",
    "3. wrap our model with DDP, which is one line of code and barely needs modification;\n",
    "4. train/test our model, which is the same as is on 1 GPU;\n",
    "5. clean up the process groups, which is one line of code;\n",
    "6. optional: gather extra data among processes (possibly needed for distributed testing), which is basically one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791b863",
   "metadata": {},
   "source": [
    "The example program in this notebook uses the [`torch.nn.parallel.DistributedDataParallel`](https://pytorch.org/docs/stable/nn.html#distributeddataparallel) class for training models in a _data parallel_ fashion: multiple workers train the same global model by processing different portions of a large dataset, computing local gradients (aka _sub_-gradients) independently and then collectively synchronizing gradients using the AllReduce primitive. In HPC terminology, this model of execution is called _Single Program Multiple Data_ or SPMD since the same application runs on all application but each one operates on different portions of the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3b4c6",
   "metadata": {},
   "source": [
    "## Setup the process group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03264334",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.distributed as dist\n",
    "\n",
    "def setup(rank, world_size):    \n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'    \n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8be74f",
   "metadata": {},
   "source": [
    "## Split the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cf18d",
   "metadata": {},
   "source": [
    "We can easily split our dataloader by `torch.utils.data.distributed.DistributedSampler`. The sampler returns an iterator over indices, which are fed into dataloader to bachify.\n",
    "\n",
    "The DistributedSampler splits the total indices of the dataset into `world_size` parts, and evenly distributes them to the dataloader in each process without duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319cac67",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def prepare(rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n",
    "    dataset = Your_Dataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, \n",
    "                            drop_last=False, shuffle=False, sampler=sampler)\n",
    "    return dataloader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24681133",
   "metadata": {},
   "source": [
    "Suppose K=3, and the length of dataset is 10. We must understand that DistributedSampler imposes even partition of indices.\n",
    "\n",
    "If we set `drop_last=False` when defining `DistributedSampler`, it will automatically pad. For example, it splits indices `[0,1,2,3,4,5,6,7,8,9]` to `[0,3,6,9]` when `rank=1`, `[0,4,7,0]` when `rank=2`, and `[2,5,8,0]` when `rank=3`. As you can see, such padding may cause issues because the padded 0 is a data record. Otherwise, it will strip off the trailing elements. For example, it splits the indices to `[0,3,6]` at `rank=1`, `[1,4,7]` at `rank=2`, and `[2,5,8]` at `rank=3`. In this case, it tailored 9 to make the indice number divisible by `world_size`.\n",
    "\n",
    "It is very simple to customize our `Sampler`. We only need to create a class, then define its `__iter__()` and `__len__()` function. Refer to the [official documentation](https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0c1a0",
   "metadata": {},
   "source": [
    "## Wrap the model with DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70cdf3d",
   "metadata": {},
   "source": [
    "We should first move our model to the specific GPU (recall that one model replica resides in one GPU), then we wrap it with DDP class. The following function takes in an argument rank, which we will introduce soon. For now, we just keep in mind rank equals the GPU id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d148c6",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def main(rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)    \n",
    "    \n",
    "    # prepare the dataloader\n",
    "    dataloader = prepare(rank, world_size)\n",
    "    \n",
    "    # instantiate the model(it's your own model) and move it to the right device\n",
    "    model = Model().to(rank)\n",
    "    \n",
    "    # wrap the model with DDP\n",
    "    # device_ids tell DDP where is your model\n",
    "    # output_device tells DDP where to output, in our case, it is rank\n",
    "    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model    \n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca0057",
   "metadata": {},
   "source": [
    "There are a few tricky things here:\n",
    "\n",
    "- When we want to access some customized attributes of the DDP wrapped model, we must reference `model.module`. That is to say, our model instance is saved as a module attribute of the DDP model. If we assign some attributes `xxx` other than built-in properties or functions, we must access them by `model.module.xxx`.\n",
    "- When we save the DDP model, our `state_dict` would add a module prefix to all parameters. \n",
    "- Consequently, if we want to load a DDP saved model to a non-DDP model, we have to manually strip the extra prefix. I provide my code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f98411",
   "metadata": {},
   "source": [
    "In case we load a DDP model checkpoint to a non-DDP `modelmodel_dict = OrderedDict()`\n",
    "\n",
    "```python\n",
    "import re\n",
    "pattern = re.compile('module.')\n",
    "\n",
    "for k,v in state_dict.items():\n",
    "    if re.search(\"module\", k):\n",
    "        model_dict[re.sub(pattern, '', k)] = v\n",
    "    else:\n",
    "        model_dict = state_dict\n",
    "        \n",
    "model.load_state_dict(model_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5ddb6",
   "metadata": {},
   "source": [
    "## Train/test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980c75a",
   "metadata": {},
   "source": [
    "This part is the key to implementing DDP. This notebook uses multi-processing to spawn the K processes (all children processes together with the parent process run the same code) but other options are possibile, i.e. the use of a distrubuted launcher `python -m torch.distributed.launch ..`.\n",
    "\n",
    "In PyTorch, `torch.multiprocessing` provides convenient ways to create parallel processes. As the official documentation says,\n",
    "\n",
    "> The spawn function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them.\n",
    "\n",
    "So, using `spawn` is a good choice.\n",
    "\n",
    "In our script, we should define a train/test function before spawning it to parallel processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07243f77",
   "metadata": {},
   "source": [
    "```python\n",
    "def main(rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)\n",
    "    # prepare the dataloader\n",
    "    dataloader = prepare(rank, world_size)\n",
    "    \n",
    "    # instantiate the model(it's your own model) and move it to the right device\n",
    "    model = Your_Model().to(rank)\n",
    "    \n",
    "    # wrap the model with DDP\n",
    "    # device_ids tell DDP where is your model\n",
    "    # output_device tells DDP where to output, in our case, it is rank\n",
    "    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model    \n",
    "    \n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)    \n",
    "    \n",
    "    #################### The above is defined previously\n",
    "   \n",
    "    optimizer = Your_Optimizer()\n",
    "    loss_fn = Your_Loss()    \n",
    "    for epoch in epochs:\n",
    "        # if we are using DistributedSampler, we have to tell it which epoch this is\n",
    "        dataloader.sampler.set_epoch(epoch)       \n",
    "        \n",
    "        for step, x in enumerate(dataloader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            pred = model(x)\n",
    "            label = x['label']\n",
    "            \n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "    cleanup()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e60d99",
   "metadata": {},
   "source": [
    "This `main` function is run in every parallel process. We now need to call it by `spawn` method. In our `.py` script, we write:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94beec65",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "world_size = 2\n",
    "mp.spawn(\n",
    "    main,\n",
    "    args=(world_size),\n",
    "    nprocs=world_size\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e023b96",
   "metadata": {},
   "source": [
    "Remember the first argument of `main` is `rank`? It is automatically passed to each process by `mp.spawn`, we donâ€™t need to pass it explicitly. `rank=0` is the master node by default. The `rank` ranges from `0` to `K-1` (2 in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340bb7f",
   "metadata": {},
   "source": [
    "## Clean up the process groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49a1dd9",
   "metadata": {},
   "source": [
    "The last line of main function is the clean up function, which is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e5182",
   "metadata": {},
   "source": [
    "```python\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d82ee8",
   "metadata": {},
   "source": [
    "## Optional: Gather extra data among processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b306c",
   "metadata": {},
   "source": [
    "Sometimes we need to collect some data from all processes, such as the testing result. We can easily gather tensors by `dist.all_gather` and objects by `dist.all_gather_object`.\n",
    "\n",
    "Without loss of generality, I assume we want to collect python objects. The only constraint of the object is it must be serializable, which is basically everything in Python. One should always assign `torch.cuda.set_device(rank)` before using `all_gather_xxx`. And, if we want to store a tensor in the object, it must locate at the `output_device`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c300971",
   "metadata": {},
   "source": [
    "```python\n",
    "def main(rank, world_size):\n",
    "    torch.cuda.set_device(rank)\n",
    "    data = {\n",
    "        'tensor': torch.ones(3,device=rank) + rank,\n",
    "        'list': [1,2,3] + rank,\n",
    "        'dict': {'rank':rank}   \n",
    "    }\n",
    "    \n",
    "    # we have to create enough room to store the collected objects\n",
    "    outputs = [None for _ in range(world_size)]\n",
    "    \n",
    "    # the first argument is the collected lists, the second argument is the data unique in each process\n",
    "    dist.all_gather_object(outputs, data)    \n",
    "    \n",
    "    # we only want to operate on the collected objects at master node\n",
    "    if rank == 0:\n",
    "        print(outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8362e4",
   "metadata": {},
   "source": [
    "## Issues about dist.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6fcf3",
   "metadata": {},
   "source": [
    "As the documentation says, `dist.barrier()` synchronizes processes. In other words, it blocks processes until all of them reaches the same line of code: `dist.barrier()`. I summarize its usage as follows:\n",
    "\n",
    "- we do not need it when training, since DDP automatically does it for us (in `loss.backward()`);\n",
    "- we do not need it when gathering data, since `dist.all_gather_object` does it for us;\n",
    "- we need it when enforcing execution order of codes, [say one process loads the model that another process saves](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) (I can hardly imagine this scenario is needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834b9fa",
   "metadata": {},
   "source": [
    "# Preparing and launching a DDP application\n",
    "Independent of how a DDP application is launched (through multi-processing or distributed launcher), each process needs a mechanism to know its global and local ranks. Once this is known, all processes create a `ProcessGroup` that enables them to participate in collective communication operations such as `AllReduce`.\n",
    "\n",
    "PyTorch has relatively simple interface for distributed training, i.e. the training script would just have to be launched using `torch.distributed.launch` or `torchrun`, or leave the code spawn multiple processes. \n",
    "\n",
    "This set of examples presents simple implementations of distributed training or message passings: \n",
    "1. CIFAR-10 classification using DistributedDataParallel wrapped ResNet models\n",
    "2. ToyModel using multiprocessing interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd10103",
   "metadata": {},
   "source": [
    "## Example 1 (Image classification using ResNet-18 over CIFAR-10)\n",
    "\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='./figs/ResNet-18-Architecture.png' width=100% /><br/>\n",
    "<b>ResNet-18 architecture</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da5495",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"code/ddp.py\"\"\"\n",
    "def main():\n",
    "\n",
    "    num_epochs_default = 5\n",
    "    batch_size_default = 256 # 1024\n",
    "    learning_rate_default = 0.1\n",
    "    random_seed_default = 0\n",
    "    model_dir_default = \"saved_models\"\n",
    "    model_filename_default = \"resnet_distributed.pth\"\n",
    "\n",
    "    # Each process runs on 1 GPU device specified by the local_rank argument.\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--num_epochs\", type=int, help=\"Number of training epochs.\", default=num_epochs_default)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Training batch size for one process.\", default=batch_size_default)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate.\", default=learning_rate_default)\n",
    "    parser.add_argument(\"--random_seed\", type=int, help=\"Random seed.\", default=random_seed_default)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=model_dir_default)\n",
    "    parser.add_argument(\"--model_filename\", type=str, help=\"Model filename.\", default=model_filename_default)\n",
    "    parser.add_argument(\"--resume\", action=\"store_true\", help=\"Resume training from saved checkpoint.\")\n",
    "    argv = parser.parse_args()\n",
    "\n",
    "    num_epochs = argv.num_epochs\n",
    "    batch_size = argv.batch_size\n",
    "    learning_rate = argv.learning_rate\n",
    "    random_seed = argv.random_seed\n",
    "    model_dir = argv.model_dir\n",
    "    model_filename = argv.model_filename\n",
    "    resume = argv.resume\n",
    "\n",
    "    if local_rank is None:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # Create directories outside the PyTorch program\n",
    "    # Do not create directory here because it is not multiprocess safe\n",
    "    '''\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    '''\n",
    "\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "    # We need to use seeds to make sure that the models initialized in different processes are the same\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    # torch.distributed.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    # Encapsulate the model on the GPU assigned to the current process\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "    model = model.to(device)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    # We only save the model who uses device \"cuda:0\"\n",
    "    # To resume, the device for the saved model would also be \"cuda:0\"\n",
    "    if resume == True:\n",
    "        map_location = {\"cuda:0\": \"cuda:{}\".format(local_rank)}\n",
    "        ddp_model.load_state_dict(torch.load(model_filepath, map_location=map_location))\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Data should be prefetched\n",
    "    # Download should be set to be False, because it is not multiprocess safe\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=True, download=True, transform=transform) \n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"/workspace/data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Restricts data loading to a subset of the dataset exclusive to the current process\n",
    "    train_sampler = DistributedSampler(dataset=train_set)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n",
    "    # Test loader does not have to follow distributed sampling strategy\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "    # Loop over the dataset multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t0 = time.time()\n",
    "        # Save and evaluate model routinely\n",
    "        if epoch % 1 == 0:\n",
    "            if local_rank == 0:\n",
    "                accuracy = evaluate(model=ddp_model, device=device, test_loader=test_loader)\n",
    "                torch.save(ddp_model.state_dict(), model_filepath)\n",
    "                print(\"-\" * 75)\n",
    "                print(\"Epoch: {}, Accuracy: {}\".format(epoch, accuracy))\n",
    "                print(\"-\" * 75)\n",
    "\n",
    "        ddp_model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Local Rank: {}, Epoch: {}, Training ...\".format(local_rank, epoch))\n",
    "        print(\"Time {} seconds\".format(round(time.time() - t0, 2)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f7fae1",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "- Set random seed to make sure that the models initialized in different processes are the same. \n",
    "- Use `DistributedDataParallel` to wrap the model for distributed training.\n",
    "- Use `DistributedSampler` to training data loader.\n",
    "- To save models, each node would save a copy of the checkpoint file in the local hard drive.\n",
    "- Downloading dataset and making directories should be avoided in the distributed training program as they are not multi-process safe, unless we use some sort of barriers, such as `torch.distributed.barrier`.\n",
    "- The node communication bandwidth are extremely important for multi-node distributed training. Instead of randomly finding two computers in the network, try to use the nodes from the specialized computing clusters, since the communications between the nodes are highly optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe6ea3",
   "metadata": {},
   "source": [
    "### Launch command 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb055fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 0, Accuracy: 0.0\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 0, Training ...\n",
      "Time 7.86 seconds\n",
      "Local Rank: 1, Epoch: 0, Training ...\n",
      "Time 7.86 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 1, Accuracy: 0.3269\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 1, Training ...\n",
      "Time 6.0 seconds\n",
      "Local Rank: 1, Epoch: 1, Training ...\n",
      "Time 6.01 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 2, Accuracy: 0.454\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 2, Training ...\n",
      "Time 5.93 seconds\n",
      "Local Rank: 1, Epoch: 2, Training ...\n",
      "Time 5.93 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 3, Accuracy: 0.4918\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 3, Training ...\n",
      "Time 6.02 seconds\n",
      "Local Rank: 1, Epoch: 3, Training ...\n",
      "Time 6.02 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 4, Accuracy: 0.5246\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 4, Training ...\n",
      "Time 5.86 seconds\n",
      "Local Rank: 1, Epoch: 4, Training ...\n",
      "Time 5.86 seconds\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1 python -W ignore -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=1234 code/ddp.py --num_epochs 5 --model_dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd8666",
   "metadata": {},
   "source": [
    "#### Launch command 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4628912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 0, Accuracy: 0.0\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 0, Training ...\n",
      "Time 7.96 seconds\n",
      "Local Rank: 1, Epoch: 0, Training ...\n",
      "Time 7.96 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 1, Accuracy: 0.3269\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 1, Training ...\n",
      "Time 5.68 seconds\n",
      "Local Rank: 1, Epoch: 1, Training ...\n",
      "Time 5.69 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 2, Accuracy: 0.454\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 2, Training ...\n",
      "Time 5.84 seconds\n",
      "Local Rank: 1, Epoch: 2, Training ...\n",
      "Time 5.82 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 3, Accuracy: 0.4918\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 3, Training ...\n",
      "Time 5.74 seconds\n",
      "Local Rank: 1, Epoch: 3, Training ...\n",
      "Time 5.75 seconds\n",
      "---------------------------------------------------------------------------\n",
      "Epoch: 4, Accuracy: 0.5246\n",
      "---------------------------------------------------------------------------\n",
      "Local Rank: 0, Epoch: 4, Training ...\n",
      "Time 5.67 seconds\n",
      "Local Rank: 1, Epoch: 4, Training ...\n",
      "Time 5.68 seconds\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nnodes=1 --nproc_per_node=2 code/ddp.py --num_epochs 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ac986",
   "metadata": {},
   "source": [
    "## Example 2 (ToyModel using multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f320e",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"mp.py\"\"\"\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e7b49",
   "metadata": {},
   "source": [
    "#### Launch command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38807865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic DDP example on rank 1.\n",
      "Running basic DDP example on rank 0.\n",
      "Running DDP with model parallel example on rank 0.\n",
      "Running DDP with model parallel example on rank 1.\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 python -W ignore code/mp.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c307e",
   "metadata": {},
   "source": [
    "**What's happen with the following command?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2ca1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Running basic DDP example on rank 1.\n",
      "Running basic DDP example on rank 0.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:561: UserWarning: For MPI backend, world_size (2) and rank (1) are ignored since they are assigned by the MPI runtime.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:561: UserWarning: For MPI backend, world_size (2) and rank (0) are ignored since they are assigned by the MPI runtime.\n",
      "  warnings.warn(\n",
      "Running basic DDP example on rank 0.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:561: UserWarning: For MPI backend, world_size (2) and rank (0) are ignored since they are assigned by the MPI runtime.\n",
      "  warnings.warn(\n",
      "Running basic DDP example on rank 1.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:561: UserWarning: For MPI backend, world_size (2) and rank (1) are ignored since they are assigned by the MPI runtime.\n",
      "  warnings.warn(\n",
      "Running DDP with model parallel example on rank 0.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:561: UserWarning: For MPI backend, world_size (2) and rank (0) are ignored since they are assigned by the MPI runtime.\n",
      "  warnings.warn(\n",
      "Running DDP with model parallel example on rank 1.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:561: UserWarning: For MPI backend, world_size (2) and rank (1) are ignored since they are assigned by the MPI runtime.\n",
      "  warnings.warn(\n",
      "Running DDP with model parallel example on rank 1.\n",
      "Running DDP with model parallel example on rank 0.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:561: UserWarning: For MPI backend, world_size (2) and rank (1) are ignored since they are assigned by the MPI runtime.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:561: UserWarning: For MPI backend, world_size (2) and rank (0) are ignored since they are assigned by the MPI runtime.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=1,2 torchrun --standalone --nnodes=1 --nproc_per_node=2 code/mp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46768e49",
   "metadata": {},
   "source": [
    "## Credits\n",
    "- [https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51](https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51)\n",
    "- [https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)\n",
    "- [https://leimao.github.io/blog/PyTorch-Distributed-Training/](https://leimao.github.io/blog/PyTorch-Distributed-Training/)\n",
    "- [https://pytorch.org/tutorials/intermediate/ddp_tutorial.html](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
    "- [https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "- [https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)\n",
    "- [https://huggingface.co/docs/transformers/performance](https://huggingface.co/docs/transformers/performance)\n",
    "- [https://github.com/pytorch/tutorials/blob/master/intermediate_source/dist_tuto.rst](https://github.com/pytorch/tutorials/blob/master/intermediate_source/dist_tuto.rst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
