{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22602516",
   "metadata": {},
   "source": [
    "# A Comprehensive Tutorial to Pytorch DistributedDataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7518d",
   "metadata": {},
   "source": [
    "The limited computation resource might discourage distibuted training across multiple gpus. It’s basically an easy job to wrap the model with DDP (short for DistributedDataParallel). \n",
    "\n",
    "In this blog, I want to share my code, my insighs with all beginners in DDP. I’m not going to include detailed explanation of how DDP works, instead, I provide minimum knowledge needed to make the model run in multiple gpus. Note that I only introduce DDP on one machine with multiple gpus, which is the most general case (Otherwise, we should use model parallel as stated in the official [blog](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html). \n",
    "\n",
    "This tutorial is organized as:\n",
    "- Overview of DDP\n",
    "- Implementation of DDP workflow (Steps 1–6)\n",
    "- Issues about dist.barrier()\n",
    "\n",
    "(https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca9e80",
   "metadata": {},
   "source": [
    "## Overview of DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931db01c",
   "metadata": {},
   "source": [
    "Terms used in distributed training:\n",
    "\n",
    "- **master node**: the main GPU responsible for synchronizations, making copies, loading models, writing checpoints and logs;\n",
    "- **process group**: if you want to train/test the model over K GPUs, then the K process forms a group, which is supported by a backend (pytorch managed that for you, according to the [documentation](https://pytorch.org/docs/1.9.0/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel), nccl is the most recommended backend);\n",
    "- **rank**: within the process group, each process is identified by its rank, from 0 to K-1;\n",
    "- **world size**: the number of processes in the group.\n",
    "\n",
    "Pytorch provides two settings for distributed training: `torch.nn.DataParallel` (DP) and `torch.nn.parallel.DistributedDataParallel` (DDP), where the latter is officially recommended. In short, DDP is faster, more flexible than DP. The fundamental thing DDP does is to copy the model to multiple gpus, gather the gradients from them, average the gradients to update the model, then synchronize the model over all K processes. \n",
    "\n",
    "We can also gather/scatter tensors/objects other than gradients by torch.distributed.gather/scatter/reduce.\n",
    "\n",
    "In case the model can fit on one GPU (it can be trained on one GPU with batch_size = 1) and we want to train/test it on K GPUs, the best practice of DDP is to copy the model onto the K GPUs (the DDP class automatically does this for you) and split the dataloader to K non-overlapping groups to feed into K models respectively.\n",
    "\n",
    "We have to do the following things:\n",
    "\n",
    "1. setup the process group, which is three lines of code and needs no modification;\n",
    "2. split the dataloader to each process in the group, which can be easily achieved by torch.utils.data.DistributedSampler or any customized sampler;\n",
    "3. wrap our model with DDP, which is one line of code and barely needs modification;\n",
    "4. train/test our model, which is the same as is on 1 GPU;\n",
    "5. clean up the process groups, which is one line of code;\n",
    "6. optional: gather extra data among processes (possibly needed for distributed testing), which is basically one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98709f",
   "metadata": {},
   "source": [
    "## Setup the process group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7956f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "\n",
    "def setup(rank, world_size):    \n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'    \n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7503011c",
   "metadata": {},
   "source": [
    "## Split the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d94c32",
   "metadata": {},
   "source": [
    "We can easily split our dataloader by `torch.utils.data.distributed.DistributedSampler`. The sampler returns an iterator over indices, which are fed into dataloader to bachify.\n",
    "\n",
    "The DistributedSampler split the total indices of the dataset into `world_size` parts, and evenly distributes them to the dataloader in each process without duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d9c3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def prepare(rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n",
    "    dataset = Your_Dataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, \n",
    "                            drop_last=False, shuffle=False, sampler=sampler)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557ff61",
   "metadata": {},
   "source": [
    "Suppose K=3, and the length of dataset is 10. We must understand that DistributedSampler imposes even partition of indices.\n",
    "\n",
    "If we set `drop_last=False` when defining `DistributedSampler`, it will automatically pad. For example, it splits indices `[0,1,2,3,4,5,6,7,8,9]` to `[0,3,6,9]` when `rank=1`, `[0,4,7,0]` when `rank=2`, and `[2,5,8,0]` when `rank=3`. As you can see, such padding may cause issues because the padded 0 is a data record. Otherwise, it will strip off the trailing elements. For example, it splits the indices to `[0,3,6]` at `rank=1`, `[1,4,7]` at `rank=2`, and `[2,5,8]` at `rank=3`. In this case, it tailored 9 to make the indice number divisible by `world_size`.\n",
    "\n",
    "It is very simple to customize our `Sampler`. We only need to create a class, then define its `__iter__()` and `__len__()` function. Refer to the [official documentation](https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler) for more details.\n",
    "\n",
    "BTW, you’d better set the `num_workers=0` when distributed training, because creating extra threads in the children processes may be problemistic. I also found `pin_memory=False` avoids many horrible bugs, maybe such things are machine-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99110949",
   "metadata": {},
   "source": [
    "## Wrap the model with DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c551b",
   "metadata": {},
   "source": [
    "We should first move our model to the specific GPU (recall that one model replica resides in one GPU), then we wrap it with DDP class. The following function takes in an argument rank, which we will introduce soon. For now, we just keep in mind rank equals the GPU id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b210fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def main(rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)    \n",
    "    \n",
    "    # prepare the dataloader\n",
    "    dataloader = prepare(rank, world_size)\n",
    "    \n",
    "    # instantiate the model(it's your own model) and move it to the right device\n",
    "    model = Model().to(rank)\n",
    "    \n",
    "    # wrap the model with DDP\n",
    "    # device_ids tell DDP where is your model\n",
    "    # output_device tells DDP where to output, in our case, it is rank\n",
    "    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model    \n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c78be52",
   "metadata": {},
   "source": [
    "There are a few tricky things here:\n",
    "\n",
    "- When we want to access some customized attributes of the DDP wrapped model, we must reference `model.module`. That is to say, our model instance is saved as a module attribute of the DDP model. If we assign some attributes `xxx` other than built-in properties or functions, we must access them by `model.module.xxx`.\n",
    "- When we save the DDP model, our state_dict would add a module prefix to all parameters. \n",
    "- Consequently, if we want to load a DDP saved model to a non-DDP model, we have to manually strip the extra prefix. I provide my code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9a01933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport re\\npattern = re.compile(\\'module.\\')\\n\\nfor k,v in state_dict.items():\\n    if re.search(\"module\", k):\\n        model_dict[re.sub(pattern, \\'\\', k)] = v\\n    else:\\n        model_dict = state_dict\\n        \\nmodel.load_state_dict(model_dict)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case we load a DDP model checkpoint to a non-DDP modelmodel_dict = OrderedDict()\n",
    "'''\n",
    "import re\n",
    "pattern = re.compile('module.')\n",
    "\n",
    "for k,v in state_dict.items():\n",
    "    if re.search(\"module\", k):\n",
    "        model_dict[re.sub(pattern, '', k)] = v\n",
    "    else:\n",
    "        model_dict = state_dict\n",
    "        \n",
    "model.load_state_dict(model_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aba757",
   "metadata": {},
   "source": [
    "## Train/test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497f4d3",
   "metadata": {},
   "source": [
    "This part is the key to implementing DDP. First we need to know the basis of multi-processing: all children processes together with the parent process run the same code.\n",
    "\n",
    "In PyTorch, `torch.multiprocessing` provides convenient ways to create parallel processes. As the official documentation says,\n",
    "\n",
    "> The spawn function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them.\n",
    "\n",
    "So, using `spawn` is a good choice.\n",
    "\n",
    "In our script, we should define a train/test function before spawning it to parallel processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e4da623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)    # prepare the dataloader\n",
    "    dataloader = prepare(rank, world_size)\n",
    "    \n",
    "    # instantiate the model(it's your own model) and move it to the right device\n",
    "    model = Your_Model().to(rank)\n",
    "    \n",
    "    # wrap the model with DDP\n",
    "    # device_ids tell DDP where is your model\n",
    "    # output_device tells DDP where to output, in our case, it is rank\n",
    "    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model    \n",
    "    \n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)    \n",
    "    \n",
    "    #################### The above is defined previously\n",
    "   \n",
    "    optimizer = Your_Optimizer()\n",
    "    loss_fn = Your_Loss()    \n",
    "    for epoch in epochs:\n",
    "        # if we are using DistributedSampler, we have to tell it which epoch this is\n",
    "        dataloader.sampler.set_epoch(epoch)       \n",
    "        \n",
    "        for step, x in enumerate(dataloader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            pred = model(x)\n",
    "            label = x['label']\n",
    "            \n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ceef44",
   "metadata": {},
   "source": [
    "This `main` function is run in every parallel process. We now need to call it by `spawn` method. In our `.py` script, we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4481e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14939/1620304947.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# suppose we have 3 gpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mworld_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     mp.spawn(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    228\u001b[0m                ' torch.multiprocessing.start_processes(...)' % start_method)\n\u001b[1;32m    229\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 )\n\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 raise ProcessExitedException(\n\u001b[0m\u001b[1;32m    140\u001b[0m                     \u001b[0;34m\"process %d terminated with exit code %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # suppose we have 3 gpus\n",
    "    world_size = 2\n",
    "    mp.spawn(\n",
    "        main,\n",
    "        args=(world_size),\n",
    "        nprocs=world_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee48a2",
   "metadata": {},
   "source": [
    "Remember the first argument of `main` is `rank`? It is automatically passed to each process by `mp.spawn`, we don’t need to pass it explicitly. `rank=0` is the master node by default. The `rank` ranges from `0` to `K-1` (2 in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c9147",
   "metadata": {},
   "source": [
    "## Clean up the process groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05094e6e",
   "metadata": {},
   "source": [
    "The last line of main function is the clean up function, which is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffde0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc65e7d",
   "metadata": {},
   "source": [
    "## Optional: Gather extra data among processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0315974",
   "metadata": {},
   "source": [
    "Sometimes we need to collect some data from all processes, such as the testing result. We can easily gather tensors by `dist.all_gather` and objects by `dist.all_gather_object`.\n",
    "\n",
    "Without loss of generality, I assume we want to collect python objects. The only constraint of the object is it must be serializable, which is basically everything in python. One should always assign `torch.cuda.set_device(rank)` before using `all_gather_xxx`. And, if we want to store a tensor in the object, it must locate at the `output_device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82b5a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size):\n",
    "    torch.cuda.set_device(rank)\n",
    "    data = {\n",
    "        'tensor': torch.ones(3,device=rank) + rank,\n",
    "        'list': [1,2,3] + rank,\n",
    "        'dict': {'rank':rank}   \n",
    "    }\n",
    "    \n",
    "    # we have to create enough room to store the collected objects\n",
    "    outputs = [None for _ in range(world_size)]\n",
    "    \n",
    "    # the first argument is the collected lists, the second argument is the data unique in each process\n",
    "    dist.all_gather_object(outputs, data)    \n",
    "    \n",
    "    # we only want to operate on the collected objects at master node\n",
    "    if rank == 0:\n",
    "        print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7254f",
   "metadata": {},
   "source": [
    "## Issues about dist.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51495fcc",
   "metadata": {},
   "source": [
    "The most confusing thing to me is when to use `dist.barrier()`. As the documentation says, it synchronizes processes. In other words, it blocks processes until all of them reaches the same line of code: `dist.barrier()`. I summarize its usage as follows:\n",
    "\n",
    "- we do not need it when training, since DDP automatically does it for us (in `loss.backward()`);\n",
    "- we do not need it when gathering data, since `dist.all_gather_object` does it for us;\n",
    "- we need it when enforcing execution order of codes, [say one process loads the model that another process saves](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) (I can hardly imagine this scenario is needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456f5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
