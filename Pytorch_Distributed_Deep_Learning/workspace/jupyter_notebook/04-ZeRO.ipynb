{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602bde7f",
   "metadata": {},
   "source": [
    "<p><center> <a href=\"../Start_Here.ipynb\"> Home Page</a> </center> </p> \n",
    "<div>\n",
    "    <span style=\"float: left; width:20%; text-align: left;\"><a href=\"05-Memory_Format.ipynb\" >Previous Notebook </a></span>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb686cd",
   "metadata": {},
   "source": [
    "## What is ZeroRedundancyOptimizer?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9df18",
   "metadata": {},
   "source": [
    "The idea of `ZeroRedundancyOptimizer` comes from [DeepSpeed/ZeRO](https://www.deepspeed.ai/tutorials/zero/) project and [Marian](https://marian-nmt.github.io/docs/) that shard optimizer states across distributed data-parallel processes to reduce per-process memory footprint. In this notebook, each process keeps a dedicated replica of the optimizer. Since DDP has already synchronized gradients in the backward pass, all optimizer replicas will operate on the same parameter and gradient values in every iteration, and this is how DDP keeps model replicas in the same state. Oftentimes, optimizers also maintain local states. For example, the Adam optimizer uses per-parameter `exp_avg` and `exp_avg_sq` states. As a result, **the Adam optimizer's memory consumption is at least twice the model size**. Given this observation, we can reduce the optimizer memory footprint by sharding optimizer states across DDP processes. More specifically, instead of creating per-param states for all parameters, each optimizer instance in different DDP processes only keeps optimizer states for a shard of all model parameters. The optimizer `step()` function only updates the parameters in its shard and then broadcasts its updated parameters to all other peer DDP processes, so that all model replicas still land in the same state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26076c8",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use ZeroRedundancyOptimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc0f99",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"zero.py\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def print_peak_memory(prefix, device):\n",
    "    if device == 0:\n",
    "        print(f\"{prefix}: {torch.cuda.max_memory_allocated(device) // 1e6}MB \")\n",
    "\n",
    "def example(rank, world_size, use_zero):\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    # create default process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # create local model\n",
    "    model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n",
    "    print_peak_memory(\"Max memory allocated after creating local model\", rank)\n",
    "\n",
    "    # construct DDP model\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    print_peak_memory(\"Max memory allocated after creating DDP\", rank)\n",
    "\n",
    "    # define loss function and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    if use_zero:\n",
    "        optimizer = ZeroRedundancyOptimizer(\n",
    "            ddp_model.parameters(),\n",
    "            optimizer_class=torch.optim.Adam,\n",
    "            lr=0.01\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(ddp_model.parameters(), lr=0.01)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = ddp_model(torch.randn(20, 2000).to(rank))\n",
    "    labels = torch.randn(20, 2000).to(rank)\n",
    "    # backward pass\n",
    "    loss_fn(outputs, labels).backward()\n",
    "\n",
    "    # update parameters\n",
    "    print_peak_memory(\"Max memory allocated before optimizer step()\", rank)\n",
    "    optimizer.step()\n",
    "    print_peak_memory(\"Max memory allocated after optimizer step()\", rank)\n",
    "\n",
    "    print(f\"params sum is: {sum(model.parameters()).sum()}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    world_size = 2\n",
    "    print(\"=== Using ZeroRedundancyOptimizer ===\")\n",
    "    mp.spawn(example,\n",
    "        args=(world_size, True),\n",
    "        nprocs=world_size,\n",
    "        join=True)\n",
    "\n",
    "    print(\"=== Not Using ZeroRedundancyOptimizer ===\")\n",
    "    mp.spawn(example,\n",
    "        args=(world_size, False),\n",
    "        nprocs=world_size,\n",
    "        join=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a47ae",
   "metadata": {},
   "source": [
    "## Launch the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7dd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1 python ../source_code/zero.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faeab04",
   "metadata": {},
   "source": [
    "---\n",
    "### References\n",
    "- https://github.com/pytorch/tutorials/blob/master/recipes_source/zero_redundancy_optimizer.rst\n",
    "- https://pytorch.org/docs/master/distributed.optim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8537b9",
   "metadata": {},
   "source": [
    "<p><center> <a href=\"../Start_Here.ipynb\"> Home Page</a> </center> </p> \n",
    "<div>\n",
    "    <span style=\"float: left; width:20%; text-align: left;\"><a href=\"05-Memory_Format.ipynb\" >Previous Notebook </a></span>\n",
    " \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
