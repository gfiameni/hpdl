{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528a7368",
   "metadata": {},
   "source": [
    "<p><center> <a href=\"../Start_Here.ipynb\"> Home Page</a> </center> </p> \n",
    "<div>\n",
    "    <span style=\"float: left; width:20%; text-align: left;\"><a href=\"02-Model_Parallelism.ipynb\" >Previous Notebook </a></span>\n",
    "    <span style=\"float: left; width:75%; text-align: right;\"><a href=\"08-Horovod.ipynb\" >Next Notebook </a></span>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37103aa3",
   "metadata": {},
   "source": [
    "## Message passing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282879a",
   "metadata": {},
   "source": [
    "### Point-to-Point Communication\n",
    "<!--\n",
    "* send/recv\n",
    "* isend/irecv\n",
    "-->\n",
    "\n",
    "![alt text](images/send_recv.png)\n",
    "\n",
    "\n",
    "A transfer of data from one process to another is called a point-to-point communication. These are achieved through the `send` and `recv` functions or their *immediate* counter-parts, `isend` and `irecv`.\n",
    "\n",
    "```python\n",
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "\n",
    "In the above example, both processes start with a zero tensor, then process 0 increments the tensor and sends it to process 1 so that they both end up with 1.0. Notice that process 1 needs to allocate memory in order to store the data it will receive.\n",
    "\n",
    "Also notice that `send`/`recv` are **blocking**: both processes stop until the communication is completed. On the other hand immediates are **non-blocking**; the script continues its execution and the methods return a `DistributedRequest` object upon which we can choose to `wait()`.\n",
    "\n",
    "```python\n",
    "\"\"\"Non-blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_non_blocking(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        req = dist.isend(tensor=tensor, dst=1)\n",
    "        print('Rank 0 started sending')\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print('Rank 1 started receiving')\n",
    "        print('Rank 1 has data ', tensor[0])\n",
    "    req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "```\n",
    "\n",
    "Running the above function might result in process 1 still having 0.0 while having already started receiving. However, after `req.wait()` has been executed we are guaranteed that the communication took place, and that the value stored in `tensor[0]` is 1.0.\n",
    "\n",
    "Point-to-point communication is useful when we want a fine-grained control over the communication of our processes. They can be used to implement fancy algorithms, such as the one used in [Baidu's DeepSpeech](https://github.com/baidu-research/baidu-allreduce) or [Facebook's large-scale experiments](https://research.fb.com/publications/imagenet1kin1h/).)\n",
    "\n",
    "### Collective Communication\n",
    "<!--\n",
    "* gather\n",
    "* reduce\n",
    "* broadcast\n",
    "* scatter\n",
    "* all_reduce\n",
    "-->\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='images/scatter.png' width=100% /><br/>\n",
    "<b>Broadcast</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='images/all_gather.png' width=100% /><br/>\n",
    "<b>AllGather</b>\n",
    "</td>\n",
    "\n",
    "</tr><tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='images/reduce.png' width=100% /><br/>\n",
    "<b>Reduce</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='images/all_reduce.png' width=100% /><br/>\n",
    "<b>AllReduce</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='images/scatter.png' width=100% /><br/>\n",
    "<b>Scatter</b>\n",
    "</td>\n",
    "\n",
    "<td align='center'>\n",
    "<img src='images/gather.png' width=100% /><br/>\n",
    "<b>Gather</b>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a **group**. A group is a subset of all our processes. To create a group, we can pass a list of ranks to `dist.new_group(group)`. By default, collectives are executed on the all processes, also known as the **world**. For example, in order to obtain the sum of all tensors at all processes, we can use the `dist.all_reduce(tensor, op, group)` collective.\n",
    "\n",
    "```python\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "def run_all_reduce(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    group = dist.new_group([0, 1]) \n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "```\n",
    "\n",
    "Since we want the sum of all tensors in the group, we use `dist.reduce_op.SUM` as the reduce operator. Generally speaking, any commutative mathematical operation can be used as an operator. Out-of-the-box, PyTorch comes with 4 such operators, all working at the element-wise level:\n",
    "\n",
    "* `dist.reduce_op.SUM`,\n",
    "* `dist.reduce_op.PRODUCT`,\n",
    "* `dist.reduce_op.MAX`,\n",
    "* `dist.reduce_op.MIN`.\n",
    "\n",
    "In addition to `dist.all_reduce(tensor, op, group)`, there are a total of 6 collectives currently implemented in PyTorch.\n",
    "\n",
    "* `dist.broadcast(tensor, src, group)`: Copies `tensor` from `src` to all other processes.\n",
    "* `dist.reduce(tensor, dst, op, group)`: Applies `op` to all `tensor` and stores the result in `dst`.\n",
    "* `dist.all_reduce(tensor, op, group)`: Same as reduce, but the result is stored in all processes.\n",
    "* `dist.scatter(tensor, src, scatter_list, group)`: Copies the $i^{\\text{th}}$ tensor `scatter_list[i]` to the $i^{\\text{th}}$ process.\n",
    "* `dist.gather(tensor, dst, gather_list, group)`: Copies `tensor` from all processes in `dst`.\n",
    "* `dist.all_gather(tensor_list, tensor, group)`: Copies `tensor` from all processes to `tensor_list`, on all processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb21997",
   "metadata": {},
   "source": [
    "## Code example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac273e",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"send_receive.py\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "\n",
    "def init_processes(rank, size, fn, backend='tcp'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run_non_blocking)) # Options: run_non_blocking, run_blocking, run_allreduce\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "```\n",
    "\n",
    "The above script spawns two processes who will each setup the distributed environment, initialize the process group (`dist.init_process_group`), and finally execute the given `run` function. \n",
    "\n",
    "The `init_processes` ensures that every process will be able to coordinate through a master, using the same ip address and port. Note that we used the TCP backend, but we could have used [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) or [Gloo](http://github.com/facebookincubator/gloo) instead. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823f2e5",
   "metadata": {},
   "source": [
    "## Launch command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fc460",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../source_code/send_receive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f83a5",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width:20%; text-align: left;\"><a href=\"02-Model_Parallelism.ipynb\" >Previous Notebook </a></span>\n",
    "    <span style=\"float: left; width:75%; text-align: right;\"><a href=\"08-Horovod.ipynb\" >Next Notebook </a></span>\n",
    "    \n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "<p><center> <a href=\"../Start_Here.ipynb\"> Home Page</a> </center> </p> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
