{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc25ccd",
   "metadata": {},
   "source": [
    "# Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013e738",
   "metadata": {},
   "source": [
    "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. The goal of Horovod is to make distributed deep learning fast and easy to use.\n",
    "(https://github.com/horovod/horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60570afd",
   "metadata": {},
   "source": [
    "## Horovod with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd4d53",
   "metadata": {},
   "source": [
    "To use Horovod with PyTorch, make the following modifications to your training script:\n",
    "\n",
    "1. Run `hvd.init()`.\n",
    "1. Pin each GPU to a single process.\n",
    "    With the typical setup of one GPU per process, set this to local rank. The first process on the server will be allocated the first GPU, the second process will be allocated the second GPU, and so forth.\n",
    "\n",
    "```python\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(hvd.local_rank())\n",
    "```\n",
    "3. Scale the learning rate by the number of workers.\n",
    "1. Effective batch size in synchronous distributed training is scaled by the number of workers. An increase in learning rate compensates for the increased batch size.\n",
    "1. Wrap the optimizer in `hvd.DistributedOptimizer`.\n",
    "\n",
    "    The distributed optimizer delegates gradient computation to the original optimizer, averages gradients using allreduce or allgather, and then applies those averaged gradients.\n",
    "\n",
    "1. Broadcast the initial variable states from rank 0 to all other processes:\n",
    "\n",
    "```python\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "```\n",
    "\n",
    "    This is necessary to ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.\n",
    "\n",
    "7. Modify your code to save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "\n",
    "    Accomplish this by guarding model checkpointing code with `hvd.rank() != 0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9dc25a",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590193de",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import horovod.torch as hvd\n",
    "\n",
    "# Initialize Horovod\n",
    "hvd.init()\n",
    "\n",
    "# Pin GPU to be used to process local rank (one GPU per process)\n",
    "torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "# Define dataset...\n",
    "train_dataset = ...\n",
    "\n",
    "# Partition dataset among workers using DistributedSampler\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)\n",
    "\n",
    "# Build model...\n",
    "model = ...\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters())\n",
    "\n",
    "# Add Horovod Distributed Optimizer\n",
    "optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "\n",
    "# Broadcast parameters from rank 0 to all other processes.\n",
    "hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "for epoch in range(100):\n",
    "   for batch_idx, (data, target) in enumerate(train_loader):\n",
    "       optimizer.zero_grad()\n",
    "       output = model(data)\n",
    "       loss = F.nll_loss(output, target)\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       if batch_idx % args.log_interval == 0:\n",
    "           print('Train Epoch: {} [{}/{}]\\tLoss: {}'.format(\n",
    "               epoch, batch_idx * len(data), len(train_sampler), loss.item()))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c7d191a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,0]<stdout>:worldsize: 1\n",
      "[1,0]<stdout>:Files already downloaded and verified\n",
      "[1,0]<stdout>:Files already downloaded and verified\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Epoch: 0, Accuracy: 0.0\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Local Rank: 0, Epoch: 0, Training ...\n",
      "[1,0]<stdout>:Time 10.2 seconds\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Epoch: 1, Accuracy: 0.3811\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Local Rank: 0, Epoch: 1, Training ...\n",
      "[1,0]<stdout>:Time 8.44 seconds\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Epoch: 2, Accuracy: 0.4349\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Local Rank: 0, Epoch: 2, Training ...\n",
      "[1,0]<stdout>:Time 8.56 seconds\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Epoch: 3, Accuracy: 0.4666\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Local Rank: 0, Epoch: 3, Training ...\n",
      "[1,0]<stdout>:Time 8.38 seconds\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Epoch: 4, Accuracy: 0.5193\n",
      "[1,0]<stdout>:---------------------------------------------------------------------------\n",
      "[1,0]<stdout>:Local Rank: 0, Epoch: 4, Training ...\n",
      "[1,0]<stdout>:Time 8.32 seconds\n",
      "[1,0]<stdout>:Time elapsed: 48 s.\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 horovodrun -np 1 python ddp_horovod.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5388c7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: horovodrun [-h] [-v] -np NP [-cb] [--disable-cache]\n",
      "                  [--start-timeout START_TIMEOUT] [--network-interface NICS]\n",
      "                  [--output-filename OUTPUT_FILENAME] [--verbose]\n",
      "                  [--config-file CONFIG_FILE] [-p SSH_PORT]\n",
      "                  [-i SSH_IDENTITY_FILE]\n",
      "                  [--fusion-threshold-mb FUSION_THRESHOLD_MB]\n",
      "                  [--cycle-time-ms CYCLE_TIME_MS]\n",
      "                  [--cache-capacity CACHE_CAPACITY]\n",
      "                  [--hierarchical-allreduce | --no-hierarchical-allreduce]\n",
      "                  [--hierarchical-allgather | --no-hierarchical-allgather]\n",
      "                  [--autotune] [--autotune-log-file AUTOTUNE_LOG_FILE]\n",
      "                  [--autotune-warmup-samples AUTOTUNE_WARMUP_SAMPLES]\n",
      "                  [--autotune-steps-per-sample AUTOTUNE_STEPS_PER_SAMPLE]\n",
      "                  [--autotune-bayes-opt-max-samples AUTOTUNE_BAYES_OPT_MAX_SAMPLES]\n",
      "                  [--autotune-gaussian-process-noise AUTOTUNE_GAUSSIAN_PROCESS_NOISE]\n",
      "                  [--min-np MIN_NP] [--max-np MAX_NP] [--slots-per-host SLOTS]\n",
      "                  [--elastic-timeout ELASTIC_TIMEOUT]\n",
      "                  [--reset-limit RESET_LIMIT]\n",
      "                  [--blacklist-cooldown-range COOLDOWN_RANGE COOLDOWN_RANGE]\n",
      "                  [--timeline-filename TIMELINE_FILENAME]\n",
      "                  [--timeline-mark-cycles] [--no-stall-check]\n",
      "                  [--stall-check-warning-time-seconds STALL_CHECK_WARNING_TIME_SECONDS]\n",
      "                  [--stall-check-shutdown-time-seconds STALL_CHECK_SHUTDOWN_TIME_SECONDS]\n",
      "                  [--mpi-threads-disable] [--mpi-args MPI_ARGS] [--tcp]\n",
      "                  [--binding-args BINDING_ARGS]\n",
      "                  [--num-nccl-streams NUM_NCCL_STREAMS]\n",
      "                  [--thread-affinity THREAD_AFFINITY]\n",
      "                  [--gloo-timeout-seconds GLOO_TIMEOUT_SECONDS]\n",
      "                  [--log-level {TRACE,DEBUG,INFO,WARNING,ERROR,FATAL}]\n",
      "                  [--log-without-timestamp | -prefix-timestamp]\n",
      "                  [-H HOSTS | -hostfile HOSTFILE | --host-discovery-script HOST_DISCOVERY_SCRIPT]\n",
      "                  [--gloo | --mpi | --jsrun]\n",
      "                  ...\n",
      "horovodrun: error: the following arguments are required: -np/--num-proc\n"
     ]
    }
   ],
   "source": [
    "! horovodrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c53a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
